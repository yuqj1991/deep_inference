#include "layer_builder.hpp"
#include <assert.h>
namespace BrixLab
{
    // Reorder the data in case the weights memory layout generated by the
    // primitive and the one provided by the user are different. In this case,
    // we create additional memory objects with internal buffers that will
    // contain the reordered data.
    template<typename DType>
    void OP_convolution_inference_forward(strNodeParam<DType> *node, graphSetLink<DType> &g_net){
        node->src_bottom_memory = g_net[node->inputs[0]]->node_param.layer_top_memory;
        if (node->conv_pdesc.src_desc() != node->src_bottom_memory.get_desc()) {
            auto temp_memory = memory(node->conv_pdesc.src_desc(), BrixLab::graph_eng);
            LOG_CHECK(temp_memory.get_data_handle() != nullptr) << "temp_memory should not be nullptr";
            std::unordered_map<int, memory> op_arg = {{DNNL_ARG_FROM, node->src_bottom_memory},
                                                      {DNNL_ARG_TO, temp_memory}};
            reorder(node->src_bottom_memory, temp_memory).execute(BrixLab::graph_stream, op_arg);
            node->src_bottom_memory = temp_memory;
        }
        if(node->hasBias){
            node->op_args = {{DNNL_ARG_SRC, node->src_bottom_memory},
                        {DNNL_ARG_WEIGHTS, node->src_weights_memory},
                        {DNNL_ARG_BIAS, node->src_bias_memory},
                        {DNNL_ARG_DST, node->layer_top_memory}};
        }else{
            node->op_args = {{DNNL_ARG_SRC, node->src_bottom_memory},
                        {DNNL_ARG_WEIGHTS, node->src_weights_memory},
                        {DNNL_ARG_DST, node->layer_top_memory}};
        }
        convolution_forward(node->conv_pdesc).execute(BrixLab::graph_stream, node->op_args);
        LOG(DEBUG_INFO)<<"conv_inference done!\n"<<std::endl;
    }

    template<typename DType>
    strLayerNode<DType> OP_convolution_layer_setup(const strParam<DType> &param){
        strLayerNode<DType> node(strNodeParam<DType>(OP_type::CONVOLUTION));
        int k_w     = param.k_w;
        int k_h     = param.k_h;
        int k_c     = param.k_c;
        int k_sX    = param.stridesX;
        int k_sY    = param.stridesY;
        int k_padXL = 0;
        int k_padXR = 0;
        int k_padYT = 0;
        int k_padYB = 0;
        QUANITIZED_TYPE quantized_type = param.quantized_type;
        memory::data_type dnnDataType = dt::f32;
        memory::data_type dnnDataBiasType = dt::f32;
        LOG(DEBUG_INFO)<<"quantized type: "<<get_quantized_type(quantized_type)
                                        <<",op: "<<get_mapped_op_string(param.op_type);
        if(quantized_type == BrixLab::QUANITIZED_TYPE::UINT8_QUANTIZED){
            dnnDataType = dt::u8;
            dnnDataBiasType = dt::s32;
        }
        dnnl::memory::format_tag data_order = tag::nchw;
        dnnl::memory::format_tag weights_order = tag::oihw;
        LOG_CHECK(param.in_shapes[0].format == param.out_shapes[0].format);
        if(param.in_shapes[0].format == TENSOR_FORMATE::NHWC){
            data_order      = tag::nhwc;
            weights_order   = tag::ohwi;
            if(param.groups > 1){
                weights_order   = tag::gohwi;
            }
        }
        
        node.node_param.inputs.resize(param.inIndexs.size());
        node.node_param.inputs      = param.inIndexs;
        LOG_CHECK(param.in_shapes.size()==1)<<"CHECK_INPUTS";
        LOG_CHECK(param.out_shapes.size()==1)<<"CHECK_OUTPUTS";
        node.node_param.in_shapes.resize(1);
        node.node_param.node_name  = param.node_name;
        int inHeight    = param.in_shapes[0].Height;
        int inChannel   = param.in_shapes[0].Channel;
        int inWidth     = param.in_shapes[0].Width;
        int inBatch     = param.in_shapes[0].Batch;
        int dilateX     = param.dilateX;
        int dilateY     = param.dilateY;
        bool dilate     = false;
        node.node_param.dilateX    = dilateX;
        node.node_param.dilateY    = dilateY;
        LOG_CHECK(dilateX >= 0)<<"CHECK dilateX >= 0";
        LOG_CHECK(dilateY >= 0)<<"CHECK dilateY >= 0";
        if(dilateX == 0 && dilateY == 0){
            dilate = false;
        }else{
            dilate = true;
        }
        int DkernelX        = 1 + (k_w - 1) * (node.node_param.dilateX + 1);
        int DkernelY        = 1 + (k_h - 1) * (node.node_param.dilateY + 1);
        int outWidth        = std::floor((inWidth - DkernelX + k_padXL + k_padXR) / k_sX) + 1;
        int outHeight       = std::floor((inHeight - DkernelY + k_padYB + k_padYT) / k_sY) + 1; 
        node.node_param.out_shapes.resize(1);
        int paramOutHeight  = param.out_shapes[0].Height;
        int paramOutWidth   = param.out_shapes[0].Width;
        if(param.padMode == PaddingType::PaddingSAME){
            outHeight       = (inHeight + k_sY - 1) / k_sY; // oh = ceil(ih / stride)
            outWidth        = (inWidth + k_sX - 1) / k_sX; // ow = ceil(iw / stride)
            int pad_width   = ARGSMAX(0, (outWidth - 1) * k_sX + DkernelX - inWidth);
            int pad_height  = ARGSMAX(0, (outHeight - 1) * k_sY + DkernelY - inHeight);
            k_padYT         = std::floor(pad_height / 2);
            k_padXL         = std::floor(pad_width / 2);
            k_padYB         = pad_height - k_padYT;
            k_padXR         = pad_width - k_padXL;
            outWidth        = std::floor((inWidth - DkernelX + pad_width) / k_sX) + 1;
            outHeight       = std::floor((inHeight - DkernelY + pad_height) / k_sY) + 1;
            
        }
        LOG_CHECK(outHeight == paramOutHeight)<< "outHeight: "<<outHeight<<", paramOutHeight: "<<paramOutHeight;
        LOG_CHECK(outWidth  == paramOutWidth)<< "outwidth: "<<outWidth<<", paramOutWidth: "<<paramOutWidth;
        node.node_param.in_shapes[0]   = param.in_shapes[0];
        node.node_param.out_shapes[0]  = param.out_shapes[0];
        node.node_param.hasBias        = param.hasBias;
        node.node_param.bottom_shape   = {inBatch, inChannel, inHeight, inWidth};
        if(param.in_shapes[0].format == TENSOR_FORMATE::NHWC){
            node.node_param.bottom_shape   = {inBatch, inHeight, inWidth, inChannel};
        }
        node.node_param.groups         = param.groups >= 1 ? param.groups : 1;
        if(node.node_param.groups > 1){
            LOG_CHECK(inChannel % node.node_param.groups == 0)<<inChannel<<","<<node.node_param.groups;
            LOG_CHECK(k_c % node.node_param.groups == 0)<<k_c<<","<<node.node_param.groups;
            int ICg             = inChannel / node.node_param.groups;
            int OCg             = k_c / node.node_param.groups;
            node.node_param.weights_shape  = {node.node_param.groups, OCg, ICg, k_h, k_w};
            if(param.in_shapes[0].format == TENSOR_FORMATE::NHWC){
                node.node_param.bottom_shape   = {node.node_param.groups, OCg, k_h, k_w, ICg};
            }
        }else if(node.node_param.groups == 1){
            node.node_param.weights_shape  = {k_c, inChannel, k_h, k_w};
            if(param.in_shapes[0].format == TENSOR_FORMATE::NHWC){
                node.node_param.weights_shape   = {k_c, k_h, k_w, inChannel};
            }
        }
        node.node_param.conv_strides       = {k_sY, k_sX};
        node.node_param.conv_paddingL      = {k_padYT, k_padXL};
        node.node_param.conv_paddingR      = {k_padYB, k_padXR};
        
        if(node.node_param.hasBias)
            node.node_param.bias_shape     = {k_c};
        
        // src bottom_md
        node.node_param.src_bottom_md = memory::desc({node.node_param.bottom_shape}, dnnDataType, data_order);
        // weights & bias
        node.node_param.src_weights_md = memory::desc({node.node_param.weights_shape}, dnnDataType, weights_order);
        if(node.node_param.groups > 1){
            node.node_param.src_weights_memory = memory({{node.node_param.weights_shape}, dnnDataType, weights_order}, BrixLab::graph_eng);
        }else if(node.node_param.groups == 1){
            node.node_param.src_weights_memory = memory({{node.node_param.weights_shape}, dnnDataType, weights_order}, BrixLab::graph_eng);
        }
        write_to_dnnl_memory(param.conv_weights, node.node_param.src_weights_memory);
        if(node.node_param.hasBias){
            node.node_param.src_bias_md = memory::desc({node.node_param.bias_shape}, dnnDataBiasType, tag::any);
            node.node_param.src_bias_memory = memory({{node.node_param.bias_shape}, dnnDataBiasType, tag::x}, BrixLab::graph_eng);
            if(quantized_type == QUANITIZED_TYPE::UINT8_QUANTIZED){
                write_to_dnnl_memory(param.quantized_bias, node.node_param.src_bias_memory);
            }else if(quantized_type == QUANITIZED_TYPE::FLOAT32_REGULAR){
                write_to_dnnl_memory(param.conv_bias, node.node_param.src_bias_memory);
            }
        }
        // output feature shape
        node.node_param.top_shape       = {inBatch, k_c, outHeight, outWidth};
        if(param.in_shapes[0].format == TENSOR_FORMATE::NHWC){
            node.node_param.top_shape   = {inBatch, outHeight, outWidth, k_c};
        }
        node.node_param.layer_top_md    = memory::desc({node.node_param.top_shape}, dnnDataType, data_order);

        LOG_CHECK(!node.node_param.src_bottom_md.is_zero())<<"CHECK src_bottom_md";
        LOG_CHECK(!node.node_param.src_bias_md.is_zero())<<"CHECK src_bias_md";
        LOG_CHECK(!node.node_param.src_weights_md.is_zero())<<"CHECK src_weights_md";
        LOG_CHECK(!node.node_param.layer_top_md.is_zero())<<"CHECK layer_top_md";
        print_dnnl_memory_shape(node.node_param.bottom_shape, "bottom_shape");
        print_dnnl_memory_shape(node.node_param.top_shape, "top_shape");
        
        if(!dilate){ 
            if(node.node_param.hasBias){
                auto convolution_desc = convolution_forward::desc(prop_kind::forward_inference,
                                        algorithm::convolution_direct, node.node_param.src_bottom_md, node.node_param.src_weights_md,
                                        node.node_param.src_bias_md, node.node_param.layer_top_md, node.node_param.conv_strides, 
                                        node.node_param.conv_paddingL, node.node_param.conv_paddingR);
                if(param.fused_ops){
                    node.node_param.conv_post_op = get_posts_opsMap(param.fused_act_type);
                    node.node_param.conv_ops.append_eltwise(node.node_param.conv_post_op.scale, 
                                                    node.node_param.conv_post_op.posts_op, 
                                                    node.node_param.conv_post_op.alpha,
                                                    node.node_param.conv_post_op.beta);
                    node.node_param.conv_attr.set_post_ops(node.node_param.conv_ops);
                    node.node_param.conv_pdesc = convolution_forward::primitive_desc(convolution_desc, node.node_param.conv_attr, BrixLab::graph_eng);
                }else{
                    node.node_param.conv_pdesc = convolution_forward::primitive_desc(convolution_desc, BrixLab::graph_eng);
                }
            }else{
                auto convolution_desc = convolution_forward::desc(prop_kind::forward_inference, 
                                        algorithm::convolution_direct, node.node_param.src_bottom_md, 
                                        node.node_param.src_weights_md, node.node_param.layer_top_md, 
                                        node.node_param.conv_strides, node.node_param.conv_paddingL, node.node_param.conv_paddingR);
                if(param.fused_ops){
                    node.node_param.conv_post_op = get_posts_opsMap(param.fused_act_type);
                    node.node_param.conv_ops.append_eltwise(node.node_param.conv_post_op.scale, 
                                                    node.node_param.conv_post_op.posts_op, 
                                                    node.node_param.conv_post_op.alpha,
                                                    node.node_param.conv_post_op.beta);
                    node.node_param.conv_attr.set_post_ops(node.node_param.conv_ops);
                    node.node_param.conv_pdesc = convolution_forward::primitive_desc(convolution_desc, node.node_param.conv_attr, BrixLab::graph_eng);
                }else{
                    node.node_param.conv_pdesc = convolution_forward::primitive_desc(convolution_desc, BrixLab::graph_eng);
                }
            }
            
        }else if(dilate){
            memory::dims conv_dilates = {node.node_param.dilateX, node.node_param.dilateX};
            if(node.node_param.hasBias){
                auto convolution_desc = convolution_forward::desc(prop_kind::forward_inference,
                                          algorithm::convolution_direct,node.node_param.src_bottom_md, node.node_param.src_weights_md,
                                          node.node_param.src_bias_md, node.node_param.layer_top_md, node.node_param.conv_strides, conv_dilates,
                                          node.node_param.conv_paddingL, node.node_param.conv_paddingR);
                if(param.fused_ops){
                    node.node_param.conv_post_op = get_posts_opsMap(param.fused_act_type);
                    node.node_param.conv_ops.append_eltwise(node.node_param.conv_post_op.scale, 
                                                    node.node_param.conv_post_op.posts_op, 
                                                    node.node_param.conv_post_op.alpha,
                                                    node.node_param.conv_post_op.beta);
                    node.node_param.conv_attr.set_post_ops(node.node_param.conv_ops);
                    node.node_param.conv_pdesc = convolution_forward::primitive_desc(convolution_desc, node.node_param.conv_attr, BrixLab::graph_eng);
                }else{
                    node.node_param.conv_pdesc = convolution_forward::primitive_desc(convolution_desc, BrixLab::graph_eng);
                }
            }else{
                auto convolution_desc = convolution_forward::desc(prop_kind::forward_inference,
                                          algorithm::convolution_direct,node.node_param.src_bottom_md, node.node_param.src_weights_md,
                                          node.node_param.layer_top_md, node.node_param.conv_strides, conv_dilates,
                                          node.node_param.conv_paddingL, node.node_param.conv_paddingR);
                if(param.fused_ops){
                    node.node_param.conv_post_op = get_posts_opsMap(param.fused_act_type);
                    node.node_param.conv_ops.append_eltwise(node.node_param.conv_post_op.scale, 
                                                    node.node_param.conv_post_op.posts_op, 
                                                    node.node_param.conv_post_op.alpha,
                                                    node.node_param.conv_post_op.beta);
                    node.node_param.conv_attr.set_post_ops(node.node_param.conv_ops);
                    node.node_param.conv_pdesc = convolution_forward::primitive_desc(convolution_desc, node.node_param.conv_attr, BrixLab::graph_eng);
                }else{
                    node.node_param.conv_pdesc = convolution_forward::primitive_desc(convolution_desc, BrixLab::graph_eng);
                }
            }    
        }

        if (node.node_param.conv_pdesc.weights_desc() != node.node_param.src_weights_memory.get_desc()) {
            auto temp_memory = memory(node.node_param.conv_pdesc.weights_desc(), BrixLab::graph_eng);
            reorder(node.node_param.src_weights_memory, temp_memory)
                    .execute(BrixLab::graph_stream, node.node_param.src_weights_memory, temp_memory);
            node.node_param.src_weights_memory = temp_memory;
        }

        node.node_param.layer_top_memory = memory(node.node_param.conv_pdesc.dst_desc(), BrixLab::graph_eng);
        node.node_param.inference_forward = OP_convolution_inference_forward; 
        LOG(DEBUG_INFO)<<"conv_set_up done!\n"<<std::endl;
        return node;
    }
    INSTANCE_LAYEROP(convolution);
    
    template<typename DType>
    void OP_batchnorm_inference_forward(strNodeParam<DType> *node, graphSetLink<DType> &g_net){
        node->src_bottom_memory = g_net[node->inputs[0]]->node_param.layer_top_memory;;
        node->op_args           = {{DNNL_ARG_SRC, node->src_bottom_memory},
                                    {DNNL_ARG_MEAN, node->batchnorm_mean_memory},
                                    {DNNL_ARG_VARIANCE, node->batchnorm_variance_memory},
                                    {DNNL_ARG_SCALE_SHIFT, node->batchnorm_scale_shift_memory},
                                    {DNNL_ARG_DST, node->src_bottom_memory}};
        batch_normalization_forward(node->batchnorm_pdesc).execute(BrixLab::graph_stream, node->op_args);
        LOG(DEBUG_INFO)<<"batchnorm_inference done!\n"<<std::endl;
    }
        
    template<typename DType>
    strLayerNode<DType> OP_batchnorm_layer_setup(const strParam<DType> &param){
        strLayerNode<DType> node(strNodeParam<DType>(OP_type::BATCHNORM));
        LOG_CHECK(param.in_shapes.size() == 1)<<"CHECK_INPUTS";
        LOG_CHECK(param.out_shapes.size() == 1)<<"CHECK_OUTPUTS";
        int inHeight        = param.in_shapes[0].Height;
        int inChannel       = param.in_shapes[0].Channel;
        int inWidth         = param.in_shapes[0].Width;
        int inBatch         = param.in_shapes[0].Batch;

        dnnl::memory::format_tag data_order = tag::nchw;
        LOG_CHECK(param.in_shapes[0].format == param.out_shapes[0].format);
        if(param.in_shapes[0].format == TENSOR_FORMATE::NHWC){
            data_order      = tag::nhwc;
        }

        node.node_param.node_name      = param.node_name;
        node.node_param.bottom_shape   = {inBatch, inChannel, inHeight, inWidth};
        node.node_param.top_shape      = {inBatch, inChannel, inHeight, inWidth};
        if(param.in_shapes[0].format == TENSOR_FORMATE::NHWC){
            node.node_param.bottom_shape   = {inBatch, inHeight, inWidth, inChannel};
            node.node_param.top_shape      = {inBatch, inHeight, inWidth, inChannel};
        }
        node.node_param.batchnorm_scale_shift_shape = {2, inChannel};
        node.node_param.in_shapes.resize(1);
        node.node_param.out_shapes.resize(1);
        node.node_param.in_shapes[0]   = param.in_shapes[0];
        node.node_param.out_shapes[0]  = param.out_shapes[0];

        node.node_param.inputs.resize(param.inIndexs.size());
        node.node_param.inputs      = param.inIndexs;

        print_dnnl_memory_shape(node.node_param.bottom_shape, "bottom_shape");
        print_dnnl_memory_shape(node.node_param.top_shape, "top_shape");

        QUANITIZED_TYPE quantized_type = param.quantized_type;
        LOG(DEBUG_INFO)<<get_quantized_type(quantized_type)
                                        <<",op: "<<get_mapped_op_string(param.op_type);
        memory::data_type dnnDataType = dt::f32;
        if(quantized_type == BrixLab::QUANITIZED_TYPE::UINT8_QUANTIZED){
            dnnDataType = dt::u8;
        }
        // src
        node.node_param.src_bottom_md                  = memory::desc(node.node_param.bottom_shape, dnnDataType, data_order);
        // scale_shift_weights
        node.node_param.batchnorm_scale_shift_md       = memory::desc(node.node_param.batchnorm_scale_shift_shape, dnnDataType, tag::nc);
        node.node_param.batchnorm_scale_shift_memory   = memory(node.node_param.batchnorm_scale_shift_md, BrixLab::graph_eng);
        write_to_dnnl_memory(param.b_shift_scale, node.node_param.batchnorm_scale_shift_memory);

        print_dnnl_memory_shape(node.node_param.bottom_shape, "bottom_shape");
        print_dnnl_memory_shape(node.node_param.top_shape, "top_shape");

        // Create operation descriptor.
        dnnl::batch_normalization_forward::desc batchnorm_desc = batch_normalization_forward::desc(
                                                                        prop_kind::forward_inference, node.node_param.src_bottom_md, 1.e-10f,
                                                                        normalization_flags::use_scale_shift
                                                                        | normalization_flags::use_global_stats);

        // Create primitive descriptor.
        node.node_param.batchnorm_pdesc = batch_normalization_forward::primitive_desc(batchnorm_desc, BrixLab::graph_eng);

        // descriptor: mean, variance.
        node.node_param.batchnorm_mean_memory = memory(node.node_param.batchnorm_pdesc.mean_desc(), BrixLab::graph_eng);
        node.node_param.batchnorm_variance_memory = memory(node.node_param.batchnorm_pdesc.variance_desc(), BrixLab::graph_eng);
        write_to_dnnl_memory(param.b_means, node.node_param.batchnorm_mean_memory);
        write_to_dnnl_memory(param.b_variance, node.node_param.batchnorm_variance_memory);
        // top memory
        node.node_param.layer_top_memory = memory(node.node_param.batchnorm_pdesc.dst_desc(), BrixLab::graph_eng);
        node.node_param.inference_forward = OP_batchnorm_inference_forward;
        LOG(DEBUG_INFO)<<"batchnorm_set_up done!\n"<<std::endl;
        return node;
    }
    INSTANCE_LAYEROP(batchnorm);

    template<typename DType>   
    void OP_pooling_inference_forward(strNodeParam<DType> *node, graphSetLink<DType> &g_net){
        node->src_bottom_memory = g_net[node->inputs[0]]->node_param.layer_top_memory;;
        node->op_args           = {{DNNL_ARG_SRC, node->src_bottom_memory},
                                   {DNNL_ARG_DST, node->layer_top_memory}};
        if(!node->p_dialiated){
            pooling_forward(node->pooling_pdesc_without_d).execute(BrixLab::graph_stream, node->op_args);
        }else if(node->p_dialiated){
            pooling_v2_forward(node->pooling_pdesc).execute(BrixLab::graph_stream, node->op_args);
        }
        LOG(DEBUG_INFO)<<"pooling inference done!\n"<<std::endl;
    }
    template<typename DType>    
    strLayerNode<DType> OP_pooling_layer_setup(const strParam<DType> &param){
        strLayerNode<DType> node(strNodeParam<DType>(OP_type::POOLING));
        LOG_CHECK(param.in_shapes.size()==1)<<"CHECK inputs";
        LOG_CHECK(param.out_shapes.size()==1)<<"CHECK outputs";
        int inHeight        = param.in_shapes[0].Height;
        int inChannel       = param.in_shapes[0].Channel;
        int inWidth         = param.in_shapes[0].Width;
        int inBatch         = param.in_shapes[0].Batch;
        int k_w             = param.p_kernelsX;
        int k_h             = param.p_kernelsY;
        int k_sX            = param.p_stridesX;
        int k_sY            = param.p_stridesY;
        int k_padYT         = 0;
        int k_padYB         = 0;
        int k_padXL         = 0;
        int k_padXR         = 0;
        int dilatedX        = param.p_dilatedX;
        int dilatedY        = param.p_dilatedY;
        int DkernelX        = 1 + (k_w - 1) * (dilatedX + 1);
        int DkernelY        = 1 + (k_h - 1) * (dilatedY + 1);
        bool dilated        = false;
        node.node_param.node_name           = param.node_name;
        node.node_param.inputs.resize(param.inIndexs.size());
        node.node_param.inputs              = param.inIndexs;
        PoolingType p_type                  = param.pooling_type;
        node.node_param.pooling_type        = get_op_mapped_pooling_type(p_type);
        node.node_param.bottom_shape        = {inBatch, inChannel, inHeight, inWidth};
        BrixLab::PaddingType pad_type       = param.pooling_padType;
        QUANITIZED_TYPE quantized_type      = param.quantized_type;
        LOG(DEBUG_INFO)<<get_quantized_type(quantized_type)
                                        <<",op: "<<get_mapped_op_string(param.op_type);
        memory::data_type dnnDataType       = dt::f32;
        if(quantized_type == BrixLab::QUANITIZED_TYPE::UINT8_QUANTIZED){
            dnnDataType = dt::u8;
        }
        dnnl::memory::format_tag data_order = tag::nchw;
        LOG_CHECK(param.in_shapes[0].format == param.out_shapes[0].format);
        if(param.in_shapes[0].format == TENSOR_FORMATE::NHWC){
            data_order      = tag::nhwc;
        }
        LOG_CHECK(dilatedX >= 0)<<"CHECK dilatedX";
        LOG_CHECK(dilatedY >= 0)<<"CHECK dilatedY";
        if(dilatedY == 0  && dilatedX == 0){
            dilated             = false;
            node.node_param.p_dialiated    = false;
        }else{
            dilated             = true;
            node.node_param.p_dialiated    = true;
        }
        int outHeight = (inHeight - DkernelY + k_padYB + k_padYT) / k_sX + 1;
        int outWidth = (inWidth - DkernelX + k_padXL + k_padXR) / k_sY + 1;;
        if(param.padMode == PaddingType::PaddingSAME){
            outHeight       = (inHeight + k_sY - 1) / k_sY; // oh = ceil(ih / stride)
            outWidth        = (inWidth + k_sX - 1) / k_sX; // ow = ceil(iw / stride)
            int pad_width   = ARGSMAX(0, (outWidth - 1) * k_sX + DkernelX - inWidth);
            int pad_height  = ARGSMAX(0, (outHeight - 1) * k_sY + DkernelY - inHeight);
            k_padYT         = std::floor(pad_height / 2);
            k_padXL         = std::floor(pad_width / 2);
            k_padYB         = pad_height - k_padYT;
            k_padXR         = pad_width - k_padXL;
            outWidth        = std::floor((inWidth - DkernelX + pad_width) / k_sX) + 1;
            outHeight       = std::floor((inHeight - DkernelY + pad_height) / k_sY) + 1;
            
        }

        node.node_param.in_shapes.resize(1);
        node.node_param.out_shapes.resize(1);
        int paramOutHeight  = param.out_shapes[0].Height;
        int paramOutWidth   = param.out_shapes[0].Width;
        LOG_CHECK(outHeight == paramOutHeight)<<"CHECK PADDING CONV OUTHEIGHT: "<<outHeight <<", "<< paramOutHeight;
        LOG_CHECK(outWidth  == paramOutWidth)<<"CHECK PADDING CONV OUTWIDTH: "<<outWidth  <<", "<< paramOutWidth;
        node.node_param.in_shapes[0]   = param.in_shapes[0];
        node.node_param.out_shapes[0]  = param.out_shapes[0];
        
        node.node_param.pooling_kernel = {k_h, k_w};
        node.node_param.pooling_strides = {k_sY, k_sX};
        node.node_param.pooling_paddingL = {k_padYT, k_padXL};
        node.node_param.pooling_paddingR = {k_padYB, k_padXR};
        node.node_param.pooling_dialiate = {dilatedX, dilatedY};
        
        node.node_param.top_shape = {inBatch, inChannel, outHeight, outWidth};
        node.node_param.layer_top_md = memory::desc(node.node_param.top_shape, dnnDataType, data_order);
        node.node_param.layer_top_memory = memory(node.node_param.layer_top_md, BrixLab::graph_eng);
        node.node_param.src_bottom_md = memory::desc(node.node_param.bottom_shape, dnnDataType, data_order);

        print_dnnl_memory_shape(node.node_param.bottom_shape, "bottom_shape");
        print_dnnl_memory_shape(node.node_param.top_shape, "top_shape");
        if(dilated){
            dnnl::pooling_v2_forward::desc pooling_desc = pooling_v2_forward::desc(prop_kind::forward_inference,
                                            node.node_param.pooling_type, node.node_param.src_bottom_md, 
                                            node.node_param.layer_top_md,
                                            node.node_param.pooling_strides, node.node_param.pooling_kernel,
                                            node.node_param.pooling_dialiate, node.node_param.pooling_paddingL, 
                                            node.node_param.pooling_paddingR);
            node.node_param.pooling_pdesc = pooling_v2_forward::primitive_desc(pooling_desc, BrixLab::graph_eng);
        }else if(!dilated){
            dnnl::pooling_forward::desc pooling_desc_without_d = pooling_forward::desc(prop_kind::forward_inference, 
                                                node.node_param.pooling_type, node.node_param.src_bottom_md, 
                                                node.node_param.layer_top_md, node.node_param.pooling_strides, 
                                                node.node_param.pooling_kernel,
                                                node.node_param.pooling_paddingL,node.node_param.pooling_paddingR);
            node.node_param.pooling_pdesc_without_d = pooling_forward::primitive_desc(pooling_desc_without_d, BrixLab::graph_eng);
        }
        node.node_param.inference_forward = OP_pooling_inference_forward;
        LOG(DEBUG_INFO)<<"pool_set_up done!\n"<<std::endl;
        return node;
    }
    INSTANCE_LAYEROP(pooling);

    template<typename DType>     
    void OP_concat_inference_forward(strNodeParam<DType> *node, graphSetLink<DType> &g_net){
        for(int ii = 0; ii < node->concat_num; ii++){
            int index                        = node->inputs[ii];
            node->concat_bottom_md[ii]       = g_net[index]->node_param.layer_top_md;
            node->concat_bottom_memory[ii]   = g_net[index]->node_param.layer_top_memory;
        }
        if(!node->inputset){
            for(int ii = 0; ii < node->concat_num; ii++){
                node->op_args.insert({DNNL_ARG_MULTIPLE_SRC + ii, node->concat_bottom_memory[ii]});
            }
            node->op_args.insert({{DNNL_ARG_DST, node->layer_top_memory}});
            node->inputset = true;
        }else{
            for(int ii = 0; ii < node->concat_num; ii++){
                node->op_args[DNNL_ARG_MULTIPLE_SRC + ii] = node->concat_bottom_memory[ii];
            }
            node->op_args[DNNL_ARG_DST] = node->layer_top_memory;
        }
        concat(node->concat_pdesc).execute(BrixLab::graph_stream, node->op_args);
        LOG(DEBUG_INFO)<<"concat inference done!\n"<<std::endl;
    }
    template<typename DType>
    strLayerNode<DType> OP_concat_layer_setup(const strParam<DType> &param){
        strLayerNode<DType> node(strNodeParam<DType>(OP_type::CONCAT));
        check_inputs_shape(param.in_shapes);
        int inHeight        = param.in_shapes[0].Height;
        int inChannel       = param.in_shapes[0].Channel;
        int inWidth         = param.in_shapes[0].Width;
        int inBatch         = param.in_shapes[0].Batch;
        node.node_param.concat_num     = param.inIndexs.size();
        node.node_param.concat_axis    = 1 ;
        node.node_param.node_name      = param.node_name;

        dnnl::memory::format_tag data_order = tag::nchw;
        LOG_CHECK(param.in_shapes[0].format == param.out_shapes[0].format);
        if(param.in_shapes[0].format == TENSOR_FORMATE::NHWC){
            data_order      = tag::nhwc;
            node.node_param.concat_axis = param.concat_axis;
        }

        QUANITIZED_TYPE quantized_type = param.quantized_type;
        LOG(DEBUG_INFO)<<get_quantized_type(quantized_type)
                                        <<",op: "<<get_mapped_op_string(param.op_type);
        memory::data_type dnnDataType = dt::f32;
        if(quantized_type == BrixLab::QUANITIZED_TYPE::UINT8_QUANTIZED){
            dnnDataType = dt::u8;
        }
        node.node_param.bottom_shape   = {inBatch, inChannel, inHeight, inWidth};
        if(param.in_shapes[0].format == TENSOR_FORMATE::NHWC){
            node.node_param.bottom_shape   = {inBatch, inHeight, inWidth, inChannel};
        }
        node.node_param.src_bottom_md  = memory::desc(node.node_param.bottom_shape, dnnDataType, data_order);
        for(int ii = 0; ii < node.node_param.concat_num; ii++){
            node.node_param.inputs.push_back(param.inIndexs[ii]);
            node.node_param.concat_bottom_md.push_back(node.node_param.src_bottom_md);
        }
        for(unsigned int ii = 0; ii <param.outIndexs.size(); ii++){
            node.node_param.outputs.push_back(param.outIndexs[ii]);
        }
        node.node_param.out_shapes.resize(1);
        node.node_param.out_shapes[0]  = param.out_shapes[0];
        node.node_param.in_shapes.resize(node.node_param.concat_num);
        for(int ii = 0; ii < node.node_param.concat_num; ii++){
            node.node_param.in_shapes[ii] = param.in_shapes[ii];
        }
        node.node_param.concat_bottom_memory.resize(node.node_param.concat_num);
        node.node_param.op_type            = param.op_type;
        node.node_param.concat_pdesc       = concat::primitive_desc(node.node_param.concat_axis, node.node_param.concat_bottom_md, BrixLab::graph_eng);
        node.node_param.layer_top_md       = node.node_param.concat_pdesc.dst_desc();
        node.node_param.top_shape          = node.node_param.layer_top_md.dims();
        print_dnnl_memory_shape(node.node_param.bottom_shape, "bottom_shape");
        print_dnnl_memory_shape(node.node_param.top_shape, "top_shape");
        node.node_param.layer_top_memory   = memory(node.node_param.layer_top_md, BrixLab::graph_eng);
        node.node_param.inference_forward  = OP_concat_inference_forward;
        LOG(DEBUG_INFO)<<"concat_set_up done!\n"<<std::endl;
        return node;
    }
    INSTANCE_LAYEROP(concat);

    template<typename DType>    
    void OP_sum_inference_forward(strNodeParam<DType> *node, graphSetLink<DType> &g_net){
        for(int ii = 0; ii < node->sum_num; ii++){
            int index           = node->inputs[ii];
            node->sum_bottom_memory[ii] = g_net[index]->node_param.layer_top_memory;
        }
        if(!node->inputset){
            for(int ii = 0; ii < node->sum_num; ii++){
                node->op_args.insert({DNNL_ARG_MULTIPLE_SRC + ii, node->sum_bottom_memory[ii]});
            }
            node->op_args.insert({DNNL_ARG_DST, node->layer_top_memory});
        }else{
            for(int ii = 0; ii < node->sum_num; ii++){
                node->op_args[DNNL_ARG_MULTIPLE_SRC + ii] = node->sum_bottom_memory[ii];
            }
            node->op_args[DNNL_ARG_DST]= node->layer_top_memory;
        }
        sum(node->sum_pdesc).execute(BrixLab::graph_stream, node->op_args);
        LOG(DEBUG_INFO)<<"sum inference done!\n"<<std::endl;
    }

    template<typename DType>    
    strLayerNode<DType> OP_sum_layer_setup(const strParam<DType> &param){
        strLayerNode<DType> node(strNodeParam<DType>(OP_type::ELTWISE));
        check_inputs_shape(param.in_shapes);
        node.node_param.sum_num = param.inIndexs.size();
        for(int ii = 0; ii < node.node_param.sum_num; ii++){
            node.node_param.inputs.push_back(param.inIndexs[ii]);
        }
        int inHeight    = param.in_shapes[0].Height;
        int inChannel   = param.in_shapes[0].Channel;
        int inWidth     = param.in_shapes[0].Width;
        int inBatch     = param.in_shapes[0].Batch;
        node.node_param.node_name  = param.node_name;
        dnnl::memory::format_tag data_order     = tag::nchw;
        LOG_CHECK(param.in_shapes[0].format == param.out_shapes[0].format);
        if(param.in_shapes[0].format == TENSOR_FORMATE::NHWC){
            data_order      = tag::nhwc;
        }
        node.node_param.bottom_shape = {inBatch, inChannel, inWidth, inHeight};
        if(param.in_shapes[0].format == TENSOR_FORMATE::NHWC){
            node.node_param.bottom_shape = {inBatch, inWidth, inHeight, inChannel};
        }
        QUANITIZED_TYPE quantized_type = param.quantized_type;
        LOG(DEBUG_INFO)<<get_quantized_type(quantized_type)
                                        <<",op: "<<get_mapped_op_string(param.op_type);
        memory::data_type dnnDataType = dt::f32;
        if(quantized_type == BrixLab::QUANITIZED_TYPE::UINT8_QUANTIZED){
            dnnDataType = dt::u8;
        }
        node.node_param.src_bottom_md = memory::desc(node.node_param.bottom_shape, dnnDataType, data_order);
        for(int ii = 0; ii < node.node_param.sum_num; ii++){
            node.node_param.sum_scale[ii] = 1.f;
            node.node_param.sum_bottom_md.push_back(node.node_param.src_bottom_md);
        }
         node.node_param.out_shapes.resize(1);
        node.node_param.out_shapes[0]  = param.out_shapes[0];
        node.node_param.in_shapes.resize(node.node_param.sum_num);
        for(int ii = 0; ii < node.node_param.sum_num; ii++){
            node.node_param.in_shapes[ii] = param.in_shapes[ii];
        }
        node.node_param.sum_bottom_memory.resize(node.node_param.sum_num);
        node.node_param.sum_pdesc = sum::primitive_desc(node.node_param.sum_scale, node.node_param.sum_bottom_md, BrixLab::graph_eng);
        node.node_param.top_shape = {inBatch, inChannel, inWidth, inHeight};
        if(param.in_shapes[0].format == TENSOR_FORMATE::NHWC){
            node.node_param.top_shape = {inBatch, inWidth, inHeight, inChannel};
        }
        node.node_param.layer_top_md        = memory::desc(node.node_param.top_shape, dt::f32, data_order);
        node.node_param.layer_top_memory    = memory(node.node_param.sum_pdesc.dst_desc(), BrixLab::graph_eng);

        print_dnnl_memory_shape(node.node_param.bottom_shape, "bottom_shape");
        print_dnnl_memory_shape(node.node_param.top_shape, "top_shape");
        
        for(int ii = 0; ii < node.node_param.sum_num; ii++){
            node.node_param.op_args.insert({DNNL_ARG_MULTIPLE_SRC + ii, node.node_param.sum_bottom_memory[ii]});
        }
        node.node_param.inference_forward = OP_sum_inference_forward;
        LOG(DEBUG_INFO)<<"sum_set_up done!\n"<<std::endl;
        return node;
    }
    INSTANCE_LAYEROP(sum);
    
    template<typename DType>
    void OP_resample_inference_forward(strNodeParam<DType> *node, graphSetLink<DType> &g_net){
        node->src_bottom_memory = g_net[node->inputs[0]]->node_param.layer_top_memory;;
        node->op_args= {{DNNL_ARG_SRC, node->src_bottom_memory},
                        {DNNL_ARG_DST, node->layer_top_memory}};
        resampling_forward(node->resample_pdesc).execute(BrixLab::graph_stream, node->op_args);
        LOG(DEBUG_INFO)<<"resample inference done!\n"<<std::endl;
    }
    template<typename DType>
    strLayerNode<DType> OP_resample_layer_setup(const strParam<DType> &param){
        strLayerNode<DType> node(strNodeParam<DType>(OP_type::RESAMPLING));
        LOG_CHECK(param.in_shapes.size()==1)<<"CHECK_INPUTS";
        LOG_CHECK(param.out_shapes.size()==1)<<"CHECK_OUTPUTS";
        int inHeight    = param.in_shapes[0].Height;
        int inChannel   = param.in_shapes[0].Channel;
        int inWidth     = param.in_shapes[0].Width;
        int inBatch     = param.in_shapes[0].Batch;

        node.node_param.inputs.resize(param.inIndexs.size());
        node.node_param.inputs          = param.inIndexs;
        node.node_param.node_name       = param.node_name;
        node.node_param.bottom_shape    = {inBatch, inChannel, inHeight, inWidth};
        node.node_param.op_type = param.op_type;

        dnnl::memory::format_tag data_order = tag::nchw;
        LOG_CHECK(param.in_shapes[0].format == param.out_shapes[0].format);
        if(param.in_shapes[0].format == TENSOR_FORMATE::NHWC){
            data_order                      = tag::nhwc;
            node.node_param.bottom_shape    = {inBatch, inHeight, inWidth, inChannel};
        }
        QUANITIZED_TYPE quantized_type = param.quantized_type;
        LOG(DEBUG_INFO)<<get_quantized_type(quantized_type)
                                        <<",op: "<<get_mapped_op_string(param.op_type);
        memory::data_type dnnDataType = dt::f32;
        if(quantized_type == BrixLab::QUANITIZED_TYPE::UINT8_QUANTIZED){
            dnnDataType = dt::u8;
        }
        node.node_param.src_bottom_md = memory::desc(node.node_param.bottom_shape, dnnDataType, data_order);
        int outHeight       = param.out_shapes[0].Height;
        int outWidth        = param.out_shapes[0].Width;
        int paramOutHeight  = param.out_shapes[0].Height;
        int paramOutWidth   = param.out_shapes[0].Width;

        LOG_CHECK(outHeight == paramOutHeight)<<outHeight<<", "<<paramOutHeight;
        LOG_CHECK(outWidth  == paramOutWidth)<<outWidth<<", "<<paramOutWidth;

        node.node_param.in_shapes.resize(1);
        node.node_param.out_shapes.resize(1);
        node.node_param.in_shapes[0]    = param.in_shapes[0];
        node.node_param.out_shapes[0]   = param.out_shapes[0];
        node.node_param.top_shape       = {inBatch, inChannel, outHeight, outWidth};
        if(param.in_shapes[0].format == TENSOR_FORMATE::NHWC){
            node.node_param.top_shape   = {inBatch, outHeight, outWidth, inChannel};
        }
        node.node_param.layer_top_md        = memory::desc(node.node_param.top_shape, dnnDataType, data_order);
        node.node_param.layer_top_memory    = memory(node.node_param.layer_top_md, BrixLab::graph_eng);

        print_dnnl_memory_shape(node.node_param.bottom_shape, "bottom_shape");
        print_dnnl_memory_shape(node.node_param.top_shape, "top_shape");

        dnnl::resampling_forward::desc resample_desc = resampling_forward::desc(prop_kind::forward_inference,
            algorithm::resampling_linear, node.node_param.src_bottom_md, node.node_param.layer_top_md);
        node.node_param.resample_pdesc = resampling_forward::primitive_desc(resample_desc, BrixLab::graph_eng);

        node.node_param.inference_forward = OP_resample_inference_forward;
        LOG(DEBUG_INFO)<<"resample_set_up done!\n"<<std::endl;
        return node;
    }
    INSTANCE_LAYEROP(resample);
    
    template<typename DType>
    void OP_deconvolution_inference_forward(strNodeParam<DType> *node, graphSetLink<DType> &g_net){
        node->src_bottom_memory = g_net[node->inputs[0]]->node_param.layer_top_memory;
        if (node->deconv_pdesc.src_desc() != node->src_bottom_memory.get_desc()) {
            auto temp_memory = memory(node->deconv_pdesc.src_desc(), BrixLab::graph_eng);
            std::unordered_map<int, memory> op_arg = {{DNNL_ARG_FROM, node->src_bottom_memory},
                    {DNNL_ARG_TO, temp_memory}};
            reorder(node->src_bottom_memory, temp_memory).execute(BrixLab::graph_stream, op_arg);
            node->src_bottom_memory = temp_memory;
        }
        node->op_args = {{DNNL_ARG_SRC, node->src_bottom_memory},
                        {DNNL_ARG_WEIGHTS, node->src_weights_memory},
                        {DNNL_ARG_BIAS, node->src_bias_memory},
                        {DNNL_ARG_DST, node->layer_top_memory}};
        deconvolution_forward(node->deconv_pdesc).execute(BrixLab::graph_stream, node->op_args);
        LOG(DEBUG_INFO)<<"deconv_inference done!\n"<<std::endl;
    }    

    template<typename DType>
    strLayerNode<DType> OP_deconvolution_layer_setup(const strParam<DType> &param){
        strLayerNode<DType> node(strNodeParam<DType>(OP_type::DECONVOLUTION));
        int k_w     = param.k_w;
        int k_h     = param.k_h;
        int k_c     = param.k_c;
        int k_sX    = param.stridesX;
        int k_sY    = param.stridesY;
        int k_padXL = 0;
        int k_padXR = 0;
        int k_padYT = 0;
        int k_padYB = 0;
        node.node_param.inputs.resize(param.inIndexs.size());
        node.node_param.inputs              = param.inIndexs;
        node.node_param.node_name           = param.node_name;
        QUANITIZED_TYPE quantized_type      = param.quantized_type;
        LOG(DEBUG_INFO)<<get_quantized_type(quantized_type)
                                        <<",op: "<<get_mapped_op_string(param.op_type);
        memory::data_type dnnDataType       = dt::f32;
        memory::data_type dnnDataBiasType   = dt::f32;
        if(quantized_type == BrixLab::QUANITIZED_TYPE::UINT8_QUANTIZED){
            dnnDataType = dt::u8;
            dnnDataBiasType = dt::s32;
        }
        LOG_CHECK(param.in_shapes.size()==1)<<"CHECK_INPUTS";
        LOG_CHECK(param.out_shapes.size()==1)<<"CHECK_OUTPUTS";
        int inHeight    = param.in_shapes[0].Height;
        int inChannel   = param.in_shapes[0].Channel;
        int inWidth     = param.in_shapes[0].Width;
        int inBatch     = param.in_shapes[0].Batch;
        node.node_param.dilateX    = param.dilateX;
        node.node_param.dilateY    = param.dilateY;
        int D_kHeight   = 1 + (k_h - 1) * (node.node_param.dilateX + 1);
        int D_kWidth    = 1 + (k_w - 1) * (node.node_param.dilateY + 1);

        int outWidth = int((inWidth - 1) * k_sX + D_kWidth - (k_padXL + k_padXR));
        int outHeight = int((inHeight - 1) * k_sY + D_kHeight - (k_padYT + k_padYB));

        int paramOutHeight  = param.out_shapes[0].Height;
        int paramOutWidth   = param.out_shapes[0].Width;

        if(param.padMode == PaddingType::PaddingSAME){
            outHeight       = inHeight * k_sY + 1 - k_sY;
            outWidth        = inWidth * k_sX + 1 - k_sX;
            int pad_width   = ARGSMAX(0, (inWidth - 1) * k_sX + D_kWidth - outWidth);
            int pad_height  = ARGSMAX(0, (inHeight - 1) * k_sY + D_kHeight - outHeight);
            k_padYT         = std::floor(pad_height / 2);
            k_padXL         = std::floor(pad_width / 2);
            k_padYB         = pad_height - k_padYT;
            k_padXR         = pad_width - k_padXL;
            outWidth = int((inWidth - 1) * k_sX + D_kWidth - (k_padXL + k_padXR));
            outHeight = int((inHeight - 1) * k_sY + D_kHeight - (k_padYT + k_padYB));
            
        }
        LOG_CHECK(outHeight == paramOutHeight)<<"CHECK PADDING TRANSPOSED_CONV OUTHEIGHT";
        LOG_CHECK(outWidth  == paramOutWidth)<<"CHECK PADDING TRANSPOSED_CONV OUTWIDTH";

        node.node_param.bottom_shape            = {inBatch, inChannel, inHeight, inWidth};
        node.node_param.weights_shape           = {k_c, inChannel, k_w, k_h};
        dnnl::memory::format_tag data_order     = tag::nchw;
        dnnl::memory::format_tag weights_order  = tag::oihw;
        LOG_CHECK(param.in_shapes[0].format == param.out_shapes[0].format);
        if(param.in_shapes[0].format == TENSOR_FORMATE::NHWC){
            data_order      = tag::nhwc;
            weights_order   = tag::ohwi;
            node.node_param.weights_shape      = {k_c, inChannel, k_w, k_h};
            node.node_param.bottom_shape       = {inBatch, inHeight, inWidth, inChannel};
            //if(param.groups > 1){
            //    weights_order   = tag::gohwi;
            //}
        }
        node.node_param.deconv_strides     = {k_sX, k_sY};
        node.node_param.deconv_paddingL    = {k_padYT, k_padXL};
        node.node_param.deconv_paddingR    = {k_padYB, k_padXR};
        node.node_param.dilateX            = param.dilateX;
        node.node_param.dilateY            = param.dilateY;
        node.node_param.hasBias            = param.hasBias;
        if(node.node_param.hasBias)
            node.node_param.bias_shape = {k_c};
        // src bottom data
        node.node_param.src_bottom_md = memory::desc({node.node_param.bottom_shape}, dnnDataType, data_order);
        // weights & bias
        node.node_param.src_weights_md = memory::desc({node.node_param.weights_shape}, dnnDataType, weights_order);
        node.node_param.src_weights_memory = memory({{node.node_param.weights_shape}, dnnDataType, weights_order}, BrixLab::graph_eng);
        write_to_dnnl_memory(param.transposed_weights, node.node_param.src_weights_memory);
        if(node.node_param.hasBias){
            node.node_param.src_bias_md = memory::desc({node.node_param.bias_shape}, dnnDataBiasType, tag::any);
            node.node_param.src_bias_memory = memory({{node.node_param.bias_shape}, dnnDataBiasType, tag::x}, BrixLab::graph_eng);
            if(quantized_type == QUANITIZED_TYPE::UINT8_QUANTIZED){
                write_to_dnnl_memory(param.quantized_bias, node.node_param.src_bias_memory);
            }else if(quantized_type == QUANITIZED_TYPE::FLOAT32_REGULAR){
                write_to_dnnl_memory(param.transposed_bias, node.node_param.src_bias_memory);
            }
        }

        
        node.node_param.top_shape = {inBatch, k_c, outHeight, outWidth};

        node.node_param.in_shapes.resize(1);
        node.node_param.out_shapes.resize(1);
        node.node_param.in_shapes[0]   = param.in_shapes[0];
        node.node_param.out_shapes[0]  = param.out_shapes[0];

        print_dnnl_memory_shape(node.node_param.bottom_shape, "bottom_shape");
        print_dnnl_memory_shape(node.node_param.top_shape, "top_shape");

        node.node_param.layer_top_md = memory::desc({node.node_param.top_shape}, dnnDataType, data_order);
        int dilate = ARGSMAX(node.node_param.dilateX, node.node_param.dilateY);
        if(dilate > 0){
            memory::dims deconv_deliatd = {node.node_param.dilateX, node.node_param.dilateY};
            if(node.node_param.hasBias){
                auto deconv_desc  = deconvolution_forward::desc(prop_kind::forward_inference, algorithm::deconvolution_direct,
                                            node.node_param.src_bottom_md, node.node_param.src_weights_md,
                                            node.node_param.src_bias_md, node.node_param.layer_top_md,
                                            node.node_param.deconv_strides, deconv_deliatd,
                                            node.node_param.deconv_paddingL,node.node_param.deconv_paddingR);
                node.node_param.deconv_pdesc = deconvolution_forward::primitive_desc(deconv_desc, BrixLab::graph_eng);

            }else{
                auto deconv_desc  = deconvolution_forward::desc(prop_kind::forward_inference, algorithm::deconvolution_direct,
                                            node.node_param.src_bottom_md, node.node_param.src_weights_md,
                                            node.node_param.layer_top_md,
                                            node.node_param.deconv_strides, deconv_deliatd,
                                            node.node_param.deconv_paddingL, node.node_param.deconv_paddingR);
                node.node_param.deconv_pdesc = deconvolution_forward::primitive_desc(deconv_desc, BrixLab::graph_eng);
            }
        }else if(dilate == 0){
            if(node.node_param.hasBias){
                auto deconv_desc  = deconvolution_forward::desc(prop_kind::forward_inference, algorithm::deconvolution_direct,
                                            node.node_param.src_bottom_md, node.node_param.src_weights_md,
                                            node.node_param.src_bias_md, node.node_param.layer_top_md,
                                            node.node_param.deconv_strides, node.node_param.deconv_paddingL,
                                            node.node_param.deconv_paddingR);
                node.node_param.deconv_pdesc = deconvolution_forward::primitive_desc(deconv_desc, BrixLab::graph_eng);
            }else{
                auto deconv_desc  = deconvolution_forward::desc(prop_kind::forward_inference, algorithm::deconvolution_direct,
                                            node.node_param.src_bottom_md, node.node_param.src_weights_md,
                                            node.node_param.layer_top_md,
                                            node.node_param.deconv_strides, node.node_param.deconv_paddingL,
                                            node.node_param.deconv_paddingR);
                node.node_param.deconv_pdesc = deconvolution_forward::primitive_desc(deconv_desc, BrixLab::graph_eng);
            }
        }

        if (node.node_param.deconv_pdesc.weights_desc() != node.node_param.src_weights_memory.get_desc()) {
            auto temp_memory = memory(node.node_param.deconv_pdesc.weights_desc(), BrixLab::graph_eng);
            reorder(node.node_param.src_weights_memory, temp_memory)
                    .execute(BrixLab::graph_stream, node.node_param.src_weights_memory, temp_memory);
            node.node_param.src_weights_memory = temp_memory;
        }
       
        node.node_param.layer_top_memory = memory(node.node_param.deconv_pdesc.dst_desc(), BrixLab::graph_eng);
        
        node.node_param.inference_forward = OP_deconvolution_inference_forward;
        LOG(DEBUG_INFO)<<"deconv_set_up done!\n"<<std::endl;
        return node;
    }
    INSTANCE_LAYEROP(deconvolution);
    
    template<typename DType>
    void OP_innerproduct_inference_forward(strNodeParam<DType> *node, graphSetLink<DType> &g_net){
        node->src_bottom_memory = g_net[node->inputs[0]]->node_param.layer_top_memory;
        node->op_args = {{DNNL_ARG_SRC, node->src_bottom_memory},
                         {DNNL_ARG_WEIGHTS, node->src_weights_memory},
                         {DNNL_ARG_BIAS, node->src_bias_memory},
                         {DNNL_ARG_DST, node->layer_top_memory}};
        inner_product_forward(node->inner_pdesc).execute(BrixLab::graph_stream, node->op_args);
        LOG(DEBUG_INFO)<<"inner_product_inference done!\n"<<std::endl;
    }
   
    template<typename DType>
    strLayerNode<DType> OP_innerproduct_layer_setup(const strParam<DType> &param){
        strLayerNode<DType> node(strNodeParam<DType>(OP_type::INNERPRODUCT));
        int k_c = param.k_c;
        LOG_CHECK(param.in_shapes.size()==1)<<"CHECK_INPUTS";
        LOG_CHECK(param.out_shapes.size()==1)<<"CHECK_OUTPUTS";
        int inHeight    = param.in_shapes[0].Height;
        int inChannel   = param.in_shapes[0].Channel;
        int inWidth     = param.in_shapes[0].Width;
        int inBatch     = param.in_shapes[0].Batch;
        node.node_param.node_name               = param.node_name;
        node.node_param.bottom_shape            = {inBatch, inChannel, inHeight, inWidth};
        node.node_param.weights_shape           = {k_c, inChannel, inHeight, inWidth};
        node.node_param.bias_shape              = {k_c};
        dnnl::memory::format_tag data_order     = tag::nchw;
        dnnl::memory::format_tag weights_order  = tag::oihw;
        LOG_CHECK(param.in_shapes[0].format == param.out_shapes[0].format);
        if(param.in_shapes[0].format == TENSOR_FORMATE::NHWC){
            data_order      = tag::nhwc;
            weights_order   = tag::ohwi;
            node.node_param.bottom_shape        = {inBatch, inHeight, inWidth, inChannel};
            node.node_param.weights_shape       = {k_c, inHeight, inWidth, inChannel};
        }
        node.node_param.inputs.resize(param.inIndexs.size());
        node.node_param.inputs              = param.inIndexs;
        QUANITIZED_TYPE quantized_type      = param.quantized_type;
        LOG(DEBUG_INFO)<<get_quantized_type(quantized_type)
                                        <<",op: "<<get_mapped_op_string(param.op_type);
        memory::data_type dnnDataType       = dt::f32;
        memory::data_type dnnDataBiasType   = dt::f32;
        if(quantized_type == BrixLab::QUANITIZED_TYPE::UINT8_QUANTIZED){
            dnnDataType         = dt::u8;
            dnnDataBiasType     = dt::s32;
        }
        // src bottom data
        node.node_param.src_bottom_md       = memory::desc({node.node_param.bottom_shape}, dnnDataType, data_order);
        // weights & bias
        node.node_param.src_weights_md      = memory::desc({node.node_param.weights_shape}, dnnDataType, weights_order);
        node.node_param.src_weights_memory  =  memory({node.node_param.weights_shape, dnnDataType, weights_order}, BrixLab::graph_eng);
        write_to_dnnl_memory(param.innerWeights, node.node_param.src_weights_memory);
        node.node_param.src_bias_md         = memory::desc({node.node_param.bias_shape}, dnnDataBiasType, tag::any);
        node.node_param.src_bias_memory     = memory({{node.node_param.bias_shape}, dnnDataBiasType, tag::x}, BrixLab::graph_eng);

        if(quantized_type == QUANITIZED_TYPE::UINT8_QUANTIZED){
            write_to_dnnl_memory(param.quantized_bias, node.node_param.src_bias_memory);
        }else if(quantized_type == QUANITIZED_TYPE::FLOAT32_REGULAR){
            write_to_dnnl_memory(param.innerBias, node.node_param.src_bias_memory);
        }

        node.node_param.top_shape = {inBatch, k_c};

        print_dnnl_memory_shape(node.node_param.bottom_shape, "bottom_shape");
        print_dnnl_memory_shape(node.node_param.top_shape, "top_shape");
        
        node.node_param.layer_top_md = memory::desc({node.node_param.top_shape}, dnnDataType, tag::any);

        dnnl::inner_product_forward::desc inner_desc = inner_product_forward::desc(prop_kind::forward_inference,
                                                                                 node.node_param.src_bottom_md, 
                                                                                 node.node_param.src_weights_md, 
                                                                                 node.node_param.src_bias_md, 
                                                                                 node.node_param.layer_top_md);
        
        if(param.fused_ops){
            node.node_param.fc_post_op = get_posts_opsMap(param.fused_act_type);
            node.node_param.fc_ops.append_eltwise(node.node_param.fc_post_op.scale, node.node_param.fc_post_op.posts_op, 
                                            node.node_param.fc_post_op.alpha, node.node_param.fc_post_op.beta);
            node.node_param.fc_attr.set_post_ops(node.node_param.fc_ops);
            node.node_param.inner_pdesc = inner_product_forward::primitive_desc(
                                                inner_desc, node.node_param.fc_attr, BrixLab::graph_eng);
        }else{
            node.node_param.inner_pdesc = inner_product_forward::primitive_desc(inner_desc, BrixLab::graph_eng);
        }

        if (node.node_param.inner_pdesc.weights_desc() != node.node_param.src_weights_memory.get_desc()) {
            auto temp_memory = memory(node.node_param.inner_pdesc.weights_desc(), BrixLab::graph_eng);
            reorder(node.node_param.src_weights_memory, temp_memory).execute(BrixLab::graph_stream, 
                                    node.node_param.src_weights_memory, temp_memory);
            node.node_param.src_weights_memory = temp_memory;
        }
        node.node_param.layer_top_memory = memory(node.node_param.inner_pdesc.dst_desc(), BrixLab::graph_eng);

        node.node_param.inference_forward = OP_innerproduct_inference_forward;
        LOG(DEBUG_INFO)<<"inner_product_setup done!\n"<<std::endl;
        return node;
    }
    INSTANCE_LAYEROP(innerproduct);

    template<typename DType>
    void OP_activation_inference_forward(strNodeParam<DType> *node, graphSetLink<DType> &g_net){
        node->src_bottom_memory = g_net[node->inputs[0]]->node_param.layer_top_memory;
        node->op_args = {
            {DNNL_ARG_SRC, node->src_bottom_memory},
            {DNNL_ARG_DST, node->layer_top_memory}
        };
        eltwise_forward(node->eltwise_pdesc).execute(BrixLab::graph_stream, node->op_args);
        LOG(DEBUG_INFO)<<"eltwise_inference done!\n"<<std::endl;
    }
    template<typename DType>
    strLayerNode<DType> OP_activation_layer_setup(const strParam<DType> &param){
        strLayerNode<DType> node(strNodeParam<DType>(OP_type::ACTIVITION));
        LOG_CHECK(param.in_shapes.size()==1)<<"CHECK_INPUTS";
        LOG_CHECK(param.out_shapes.size()==1)<<"CHECK_OUTPUTS";
        int inHeight    = param.in_shapes[0].Height;
        int inChannel   = param.in_shapes[0].Channel;
        int inWidth     = param.in_shapes[0].Width;
        int inBatch     = param.in_shapes[0].Batch;
        node.node_param.inputs.resize(param.inIndexs.size());
        node.node_param.inputs          = param.inIndexs;
        node.node_param.node_name       = param.node_name;
        //bottom_src memory
        QUANITIZED_TYPE quantized_type  = param.quantized_type;
        LOG(DEBUG_INFO)<<get_quantized_type(quantized_type)
                                        <<",op: "<<get_mapped_op_string(param.op_type);
        memory::data_type dnnDataType = dt::f32;
        if(quantized_type == BrixLab::QUANITIZED_TYPE::UINT8_QUANTIZED){
            dnnDataType = dt::u8;
        }
        dnnl::memory::format_tag data_order = tag::nchw;
        LOG_CHECK(param.in_shapes[0].format == param.out_shapes[0].format);
        if(param.in_shapes[0].format == TENSOR_FORMATE::NHWC){
            data_order      = tag::nhwc;
        }
        node.node_param.bottom_shape    = {inBatch, inChannel, inHeight, inWidth};
        if(param.in_shapes[0].format == TENSOR_FORMATE::NHWC){
            node.node_param.bottom_shape    = {inBatch, inHeight, inWidth, inChannel};
        }
        node.node_param.src_bottom_md   = memory::desc(node.node_param.bottom_shape, dnnDataType, data_order);

        node.node_param.in_shapes.resize(1);
        node.node_param.out_shapes.resize(1);
        node.node_param.in_shapes[0]   = param.in_shapes[0];
        node.node_param.out_shapes[0]  = param.out_shapes[0];
        
        // top &memory
        node.node_param.top_shape           = {inBatch, inChannel, inHeight, inWidth};
        if(param.in_shapes[0].format == TENSOR_FORMATE::NHWC){
            node.node_param.top_shape       = {inBatch, inHeight, inWidth, inChannel};
        }
        node.node_param.layer_top_md        = memory::desc(node.node_param.top_shape, dnnDataType, data_order);
        node.node_param.layer_top_memory    = memory(node.node_param.layer_top_md, BrixLab::graph_eng);

        print_dnnl_memory_shape(node.node_param.bottom_shape, "bottom_shape");
        print_dnnl_memory_shape(node.node_param.top_shape, "top_shape");

        //op
        node.node_param.activate_type   = get_op_mapped_activition_type(param.activate_type);
        node.node_param.alpha           = param.alpha;
        node.node_param.beta            = param.beta;
        dnnl::eltwise_forward::desc eltwise_desc = eltwise_forward::desc(prop_kind::forward_inference,
                                                        node.node_param.activate_type, node.node_param.src_bottom_md, 
                                                        node.node_param.alpha, node.node_param.beta);
        node.node_param.eltwise_pdesc = eltwise_forward::primitive_desc(eltwise_desc, BrixLab::graph_eng);
        LOG(DEBUG_INFO)<<"eltwise_setup done!\n"<<std::endl;
        return node;
    }
    INSTANCE_LAYEROP(activation);

    template<typename DType>
    void OP_binary_inference_forward(strNodeParam<DType> *node, graphSetLink<DType> &g_net){
        node->binary_memory[0]  = g_net[node->inputs[0]]->node_param.layer_top_memory;
        node->binary_memory[1]  = g_net[node->inputs[1]]->node_param.layer_top_memory;
        node->op_args           = {{DNNL_ARG_SRC_0, node->binary_memory[0]},
                                   {DNNL_ARG_SRC_1, node->binary_memory[1]},
                                   {DNNL_ARG_DST, node->layer_top_memory}};
        binary(node->binary_pdesc).execute(BrixLab::graph_stream, node->op_args);
        LOG(DEBUG_INFO)<<"binary inference done!\n"<<std::endl;
    }
    template<typename DType>
    strLayerNode<DType> OP_binary_layer_setup(const strParam<DType> &param){
        strLayerNode<DType> node(strNodeParam<DType>(OP_type::BINARY_OP));
        LOG_CHECK(param.in_shapes.size()==2)<<"CHECK_INPUTS";
        LOG_CHECK(param.out_shapes.size()==1)<<"CHECK_OUTPUTS";
        int inHeight    = param.in_shapes[0].Height;
        int inChannel   = param.in_shapes[0].Channel;
        int inWidth     = param.in_shapes[0].Width;
        int inBatch     = param.in_shapes[0].Batch;
        node.node_param.node_name           = param.node_name;
        //bottom_src memory
        QUANITIZED_TYPE quantized_type      = param.quantized_type;
        LOG(DEBUG_INFO)<<get_quantized_type(quantized_type)
                                        <<",op: "<<get_mapped_op_string(param.op_type);
        memory::data_type dnnDataType       = dt::f32;
        if(quantized_type == BrixLab::QUANITIZED_TYPE::UINT8_QUANTIZED){
            dnnDataType = dt::u8;
        }

        dnnl::memory::format_tag data_order = tag::nchw;
        LOG_CHECK(param.in_shapes[0].format == param.out_shapes[0].format);
        if(param.in_shapes[0].format == TENSOR_FORMATE::NHWC){
            data_order      = tag::nhwc;
        }
        node.node_param.bottom_shape        = {inBatch, inChannel, inHeight, inWidth};
        if(param.in_shapes[0].format == TENSOR_FORMATE::NHWC){
            node.node_param.bottom_shape        = {inBatch, inHeight, inWidth, inChannel};
        }
        node.node_param.src_bottom_md   = memory::desc(node.node_param.bottom_shape, dnnDataType, data_order);
        LOG_CHECK(param.inIndexs.size()==2)<<"CHECK 2 INPUTS";
        for(unsigned int ii = 0; ii < param.inIndexs.size(); ii++){
            LOG(DEBUG_INFO)<<"add index: "<<param.inIndexs[ii];
            node.node_param.inputs.push_back(param.inIndexs[ii]);
            node.node_param.binary_md.push_back(node.node_param.src_bottom_md);
        }
        node.node_param.binary_memory.resize(2);
        node.node_param.in_shapes.resize(2);
        node.node_param.out_shapes.resize(1);
        node.node_param.in_shapes[0]   = param.in_shapes[0];
        node.node_param.in_shapes[1]   = param.in_shapes[1];
        node.node_param.out_shapes[0]  = param.out_shapes[0];
        // top &memory
        node.node_param.top_shape           = {inBatch, inChannel, inHeight, inWidth};
        if(param.in_shapes[0].format == TENSOR_FORMATE::NHWC){
            node.node_param.top_shape       = {inBatch, inHeight, inWidth, inChannel};
        }
        node.node_param.layer_top_md        = memory::desc(node.node_param.top_shape, dnnDataType, data_order);
        node.node_param.layer_top_memory    = memory(node.node_param.layer_top_md, BrixLab::graph_eng);

        print_dnnl_memory_shape(node.node_param.bottom_shape, "bottom_shape");
        print_dnnl_memory_shape(node.node_param.top_shape, "top_shape");

        //op
        node.node_param.binary_type         = get_op_mapped_binary_type(param.binary_type);
        dnnl::binary::desc binary_desc      = binary::desc(node.node_param.binary_type, node.node_param.binary_md[0], node.node_param.binary_md[1], node.node_param.layer_top_md);
        node.node_param.binary_pdesc        = binary::primitive_desc(binary_desc, BrixLab::graph_eng);
        node.node_param.inference_forward   = OP_binary_inference_forward;
        LOG(DEBUG_INFO)<<"binary_inference done!\n"<<std::endl;
        return node;
    }
    INSTANCE_LAYEROP(binary);

    template<typename DType>
    NetGraph<DType>::NetGraph(const int &inH, const int &inW, const int &size, 
                const std::string &tflite_path, const memory &input):input_w(inW), input_h(inH), 
                graph_state(graphSetLink<DType>(0, 0, input)), graph_size(size),_tflite_model(nullptr), 
                tflite_file(tflite_path){
    }

    template<typename DType>
    int NetGraph<DType>::get_graph_size() const{
        return graph_size;
    }

    template<typename DType>
    int NetGraph<DType>::get_GraphinWidth() const{
        return input_w;
    }

    template<typename DType>
    int NetGraph<DType>::get_GraphinHeight() const{
        return input_h;
    }

    template<typename DType>
    void NetGraph<DType>::network_predict(){
        strLayerNode<DType> *layer_node    = graph_state.head;
        while(layer_node != nullptr){
            if(layer_node->node_param.op_type == OP_type::DATA_INPUTS){
                LOG(DEBUG_INFO)<<"DATA_INPUTS";
                layer_node->node_param.layer_top_memory = graph_state.input;
                layer_node                              = layer_node->next;
            }
            LOG(DEBUG_INFO)<<layer_node->node_param.node_name<<": start inference!";
            layer_node->node_param.inference_forward(&(layer_node->node_param), graph_state);
            layer_node                                  = layer_node->next;
        }
    }

    template<typename DType>
    strLayerNode<DType>* NetGraph<DType>::getGraphOutput(){
        return graph_state.tail;
    }

    template<typename DType>
    void NetGraph<DType>::make_graph(const NetT<DType>& g_net){
        const auto layerList    = g_net.layer_ops;
        for(unsigned int ii = 0; ii < layerList.size(); ii++){
            strParam<DType> param = layerList[ii];
            std::string OP_name = get_mapped_op_string(param.op_type) + "_layer_setup";
            strLayerNode<DType> node(param.op_type);
            if(param.op_type == OP_type::DATA_INPUTS){
                graph_insert(graph_state, &node);
                continue;
            }
            if(param.quantized_type == BrixLab::QUANITIZED_TYPE::FLOAT32_REGULAR){
                node = getSetupFunc(OP_name)(param);
                graph_insert(graph_state, &node);
            }else{
                LOG(FATAL_ERROR)<<"no support quantized_type: "<<OP_name;
            }            
        }
    }

    template<typename DType>
    void NetGraph<DType>::make_netParamfromTflite(const std::string &tflite_file){

        std::ifstream inputFile(tflite_file, std::ios::binary);
        inputFile.seekg(0, std::ios::end);
        const auto size = inputFile.tellg();
        inputFile.seekg(0, std::ios::beg);

        char* buffer = new char[size];
        inputFile.read(buffer, size);
        inputFile.close();

        // verify model
        flatbuffers::Verifier verify((uint8_t*)buffer, size);
        if (!tflite::VerifyModelBuffer(verify)) {
            std::cout << "TFlite model version ERROR!";
        }

        _tflite_model = tflite::UnPackModel(buffer);
        delete[] buffer;
    }

    template<typename DType>
    void NetGraph<DType>::printf_netGraph(){
        LOG_CHECK(graph_state.graph_size>0)<<"graph_state should has layer_nodes";
        LOG(DEBUG_INFO)<<"graph_size: "<<graph_state.graph_size;
        strLayerNode<DType> *layer_node    = graph_state.head;  
        while(layer_node != nullptr){
            OP_type type        = layer_node->node_param.op_type;
            std::string OP_name = get_mapped_op_string(type);
            LOG(DEBUG_INFO)<<"node op name: "<<OP_name<<", layer name: "<<layer_node->node_param.node_name.c_str();
            layer_node          = layer_node->next;
        }
    }

    template<typename DType>
    NetT<DType> NetGraph<DType>::tfliteConvertGraphList(){
        if(_tflite_model == nullptr){
            make_netParamfromTflite(tflite_file);
        }
        NetT<DType> g_net;
        const auto& tfliteOpSet = _tflite_model->operator_codes;
        const auto subGraphsSize      = _tflite_model->subgraphs.size();
        const auto& tfliteModelBuffer = _tflite_model->buffers;

        // check whether this tflite model is quantization model
        // use the weight's data type of Conv2D|DepthwiseConv2D to decide quantizedModel mode
        bool quantizedModel = true;
        for (unsigned int i = 0; i < subGraphsSize; ++i) {
            const auto& ops     = _tflite_model->subgraphs[i]->operators;
            const auto& tensors = _tflite_model->subgraphs[i]->tensors;
            const int opNums    = ops.size();
            for (int j = 0; j < opNums; ++j) {
                const int opcodeIndex = ops[j]->opcode_index;
                const auto opCode     = tfliteOpSet[opcodeIndex]->builtin_code;
                if (opCode == tflite::BuiltinOperator_CONV_2D || opCode == tflite::BuiltinOperator_DEPTHWISE_CONV_2D) {
                    const int weightIndex    = ops[j]->inputs[1];
                    const auto& weightTensor = tensors[weightIndex];
                    quantizedModel           = weightTensor->type == tflite::TensorType_UINT8;
                    if(weightTensor->type == tflite::TensorType_INT8){
                        LOG(FATAL_ERROR)<< "***DO NOT SUPPORT Tflite [INT8] quantized model now***";
                    }
                    if (!quantizedModel)
                        break;
                }
            }
        }
        //auto& buffers = _tflite_model->buffers;
        for (unsigned int i = 0; i < subGraphsSize; ++i) {
            const auto& ops     = _tflite_model->subgraphs[i]->operators;
            const auto& tensors = _tflite_model->subgraphs[i]->tensors;
            std::vector<std::string> tensor_names;
            tensor_names.resize(tensors.size());
            // set const
            std::vector<bool> extractedTensors(_tflite_model->subgraphs[i]->tensors.size(), false);
            // set input, maybe the inputs size should be 1 in one subgraphs.
            for (const auto index : _tflite_model->subgraphs[i]->inputs) {
                strParam<DType> input_OpT;
                const auto& inputTensor = tensors[index];
                input_OpT.in_shapes.resize(1);
                input_OpT.node_name     = inputTensor->name;
                input_OpT.op_type       = OP_type::DATA_INPUTS;
                input_OpT.in_shapes[0].format      = TENSOR_FORMATE::NHWC;
                input_OpT.in_shapes[0].Batch       = inputTensor->shape[0];
                input_OpT.in_shapes[0].Channel     = inputTensor->shape[3];
                input_OpT.in_shapes[0].Height      = inputTensor->shape[1];
                input_OpT.in_shapes[0].Width       = inputTensor->shape[2];
                g_net.layer_ops.emplace_back(input_OpT);
            }
            // set output names
            for (unsigned int k = 0; k < _tflite_model->subgraphs[i]->outputs.size(); ++k) {
                g_net.output_name.push_back(tensors[_tflite_model->subgraphs[i]->outputs[k]]->name);
            }
            // tensor names
            for (const auto& tensor : tensors) {
                g_net.tensorName.push_back(tensor->name);
            }
            const int opNums = ops.size();
            for (int j = 0; j < opNums; ++j) {
                const int opcodeIndex = ops[j]->opcode_index;
                const auto opCode     = tfliteOpSet[opcodeIndex]->builtin_code;
                strParam<DType> New_OP;
                auto creator = liteOpConvertMapKit<DType>::get()->search(opCode);
                LOG_CHECK(creator) << "NOT_SUPPORTED_OP: [ " << tflite::EnumNameBuiltinOperator(opCode) << " ]";

                // tflite op to onednn op
                New_OP.node_name    = tensors[ops[j]->outputs[0]]->name;
                New_OP.op_type      = creator->opType(quantizedModel);
                
                // set default input output index
                New_OP.inIndexs.resize(ops[j]->inputs.size());
                New_OP.outIndexs.resize(ops[j]->outputs.size());
                for (unsigned int i = 0; i < ops[j]->inputs.size(); i++) {
                    New_OP.inIndexs[i]  = ops[j]->inputs[i];
                }
                for (unsigned int i = 0; i < ops[j]->outputs.size(); i++) {
                    New_OP.outIndexs[i] = ops[j]->outputs[i];
                }
                // Run actual conversion
                creator->run(&New_OP, ops[j], tensors, tfliteModelBuffer, tfliteOpSet, quantizedModel);
                g_net.layer_ops.emplace_back(New_OP);
            }

            for(unsigned int i = 0; i < tensors.size(); i++){
                tensor_names[i] = tensors[i]->name;
            }

            for(unsigned int i = 0; i < g_net.layer_ops.size(); i++){
                std::vector<int>temp_index;
                temp_index.resize(g_net.layer_ops[i].inIndexs.size());
                temp_index      = g_net.layer_ops[i].inIndexs;
                for(unsigned int ii = 0; ii <temp_index.size();ii++){
                    g_net.layer_ops[i].inIndexs[ii]  = get_net_index_by_name(g_net.layer_ops, tensor_names[temp_index[ii]]);
                }
                temp_index.resize(g_net.layer_ops[i].outIndexs.size());
                temp_index      = g_net.layer_ops[i].outIndexs;
                for(unsigned int ii = 0; ii <temp_index.size();ii++){
                    g_net.layer_ops[i].outIndexs[ii]  = get_net_index_by_name(g_net.layer_ops, tensor_names[temp_index[ii]]);
                }
            }
        }
        
        return g_net;
    }

    template<typename DType>
    NetGraph<DType>::~NetGraph(){
        
    }

    INSTANEC_CLASSNET(NetGraph);

    LayerFloatSetup getSetupFunc(const std::string &func_name){
        if(func_name == "OP_convolution_layer_setup"){
            return OP_convolution_layer_setup;
        }else if(func_name == "OP_deconvolution_layer_setup"){
            return OP_deconvolution_layer_setup;
        }else if(func_name == "OP_activation_layer_setup"){
            return OP_activation_layer_setup;
        }else if(func_name == "OP_innerproduct_layer_setup"){
            return OP_innerproduct_layer_setup;
        }else if(func_name == "OP_resample_layer_setup"){
            return OP_resample_layer_setup;
        }else if(func_name == "OP_sum_layer_setup"){
            return OP_sum_layer_setup;
        }else if(func_name == "OP_concat_layer_setup"){
            return OP_concat_layer_setup;
        }else if(func_name == "OP_pooling_layer_setup"){
            return OP_pooling_layer_setup;
        }else if(func_name == "OP_batchnorm_layer_setup"){
            return OP_batchnorm_layer_setup;
        }else if(func_name == "OP_binary_layer_setup"){
            return OP_binary_layer_setup;
        }else{
            LOG(FATAL_ERROR)<<"NO SUPPORT OPS";
            return nullptr;
        }
    }

    LayerUint8Setup getSetupUintFunc(const std::string &func_name){
        if(func_name == "OP_convolution_layer_setup"){
            return OP_convolution_layer_setup;
        }else if(func_name == "OP_deconvolution_layer_setup"){
            return OP_deconvolution_layer_setup;
        }else if(func_name == "OP_activation_layer_setup"){
            return OP_activation_layer_setup;
        }else if(func_name == "OP_innerproduct_layer_setup"){
            return OP_innerproduct_layer_setup;
        }else if(func_name == "OP_resample_layer_setup"){
            return OP_resample_layer_setup;
        }else if(func_name == "OP_sum_layer_setup"){
            return OP_sum_layer_setup;
        }else if(func_name == "OP_concat_layer_setup"){
            return OP_concat_layer_setup;
        }else if(func_name == "OP_pooling_layer_setup"){
            return OP_pooling_layer_setup;
        }else if(func_name == "OP_batchnorm_layer_setup"){
            return OP_batchnorm_layer_setup;
        }else if(func_name == "OP_binary_layer_setup"){
            return OP_binary_layer_setup;
        }else{
            LOG(FATAL_ERROR)<<"NO SUPPORT OPS";
            return nullptr;
        }
    }

} // namespace BrixLab

