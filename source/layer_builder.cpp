#include "layer_builder.hpp"
#include <assert.h>
#ifdef USE_DEBUG
#include "oneapi/dnnl_debug.h"
#endif
#define USE_DUMP_FILE
namespace BrixLab
{
    // Reorder the data in case the weights memory layout generated by the
    // primitive and the one provided by the user are different. In this case,
    // we create additional memory objects with internal buffers that will
    // contain the reordered data.
    template<typename DType>
    void OP_convolution_inference_forward(strNodeParam<DType> *node, graphSetLink<DType> &g_net){
        auto temp_memory            = memory(g_net[node->inputs[0]]->node_param.top_memory.get_desc(), BrixLab::graph_eng);
        if (node->conv_pdesc.src_desc() != temp_memory.get_desc()) {
            float* src_data         = (float*)g_net[node->inputs[0]]->node_param.top_memory.get_data_handle();
            write_to_dnnl_memory(src_data, temp_memory);
            node->src_bottom_memory = memory(node->conv_pdesc.src_desc(), BrixLab::graph_eng);
            LOG_CHECK(node->src_bottom_memory.get_data_handle() != nullptr) << "CHECK SRC BOTTOM MEMOEY ERROR";
            LOG_CHECK(temp_memory.get_data_handle() != nullptr) << "CHECK TEMP MEMORY ERROR";
            reorder(temp_memory, node->src_bottom_memory).execute(BrixLab::graph_stream, 
                                                                    {{DNNL_ARG_SRC, temp_memory},
                                                                    {DNNL_ARG_DST, node->src_bottom_memory}});
        }else{
            node->src_bottom_memory = memory(g_net[node->inputs[0]]->node_param.top_memory);
        }
        if(node->hasBias){
            node->op_args = {{DNNL_ARG_SRC, node->src_bottom_memory},
                            {DNNL_ARG_WEIGHTS, node->src_weights_memory},
                            {DNNL_ARG_BIAS, node->src_bias_memory},
                            {DNNL_ARG_DST, node->top_memory}};
        }else{
            node->op_args = {{DNNL_ARG_SRC, node->src_bottom_memory},
                            {DNNL_ARG_WEIGHTS, node->src_weights_memory},
                            {DNNL_ARG_DST, node->top_memory}};
                            
        }
        convolution_forward(node->conv_pdesc).execute(BrixLab::graph_stream, node->op_args);
        #ifdef USE_DUMP_FILE
        float *dump_dst = (float*)node->top_memory.get_data_handle();
        size_t size     = node->top_memory.get_desc().get_size();
        dump_data_to_file(dump_dst, size / 4, node->node_name.c_str(), node->top_memory.get_desc().dims());
        #endif
        LOG(DEBUG_INFO)<<"conv_inference done!\n"<<std::endl;
    }

    template<typename DType>
    strLayerNode<DType> OP_convolution_layer_setup(const strParam<DType> &param){
        strLayerNode<DType> node(strNodeParam<DType>(OP_type::CONVOLUTION));
        LOG(DEBUG_INFO)<<"quantized type: "<<get_quantized_type(param.quantized_type)<<",op: "<<get_mapped_op_string(param.op_type);
        // node_name
        node.node_param.node_name           = param.node_name;
        node.node_param.op_type             = param.op_type;
        // quantized_type
        QUANITIZED_TYPE quantized_type      = param.quantized_type;
        memory::data_type dnnDataType       = dt::f32;
        memory::data_type biasDataType   = dt::f32;
        if(quantized_type == BrixLab::QUANITIZED_TYPE::UINT8_QUANTIZED){
            dnnDataType = dt::u8;
            biasDataType = dt::s32;
        }
        //data format nhwc or nchw 
        LOG_CHECK(param.in_shapes[0].format == param.out_shapes[0].format);
        dnnl::memory::format_tag data_order     = tag::nchw;
        dnnl::memory::format_tag weights_order  = tag::oihw;
        if(param.groups > 1){
            weights_order                       = tag::goihw;
        }
        //inIndexes
        node.node_param.inputs.resize(param.inIndexs.size());
        node.node_param.inputs          = param.inIndexs;

        LOG_CHECK(param.in_shapes.size() == 1)<<"CHECK_INPUTS SIZE ERROR";
        LOG_CHECK(param.out_shapes.size()== 1)<<"CHECK_OUTPUTS SIZE ERROR";
        node.node_param.in_shapes.resize(1);
        node.node_param.in_shapes[0]    = param.in_shapes[0];
        node.node_param.out_shapes.resize(1);
        node.node_param.out_shapes[0]   = param.out_shapes[0];
        int inHeight    = param.in_shapes[0].Height;
        int inChannel   = param.in_shapes[0].Channel;
        int inWidth     = param.in_shapes[0].Width;
        int inBatch     = param.in_shapes[0].Batch;
        int k_w         = param.k_w;
        int k_h         = param.k_h;
        int k_c         = param.k_c;
        int k_sX        = param.stridesX;
        int k_sY        = param.stridesY;
        int k_padXL     = 0;
        int k_padXR     = 0;
        int k_padYT     = 0;
        int k_padYB     = 0;
        int dilateX     = param.dilateX;
        int dilateY     = param.dilateY;
        bool dilate     = false;
        node.node_param.dilateX    = dilateX;
        node.node_param.dilateY    = dilateY;
        LOG_CHECK(dilateX >= 0)<<"CHECK dilateX >= 0";
        LOG_CHECK(dilateY >= 0)<<"CHECK dilateY >= 0";
        if(dilateX == 0 && dilateY == 0){
            dilate = false;
        }else{
            dilate = true;
        }
        int DkernelX        = 1 + (k_w - 1) * (node.node_param.dilateX + 1);
        int DkernelY        = 1 + (k_h - 1) * (node.node_param.dilateY + 1);
        int outWidth        = std::floor((inWidth - DkernelX + k_padXL + k_padXR) / k_sX) + 1;
        int outHeight       = std::floor((inHeight - DkernelY + k_padYB + k_padYT) / k_sY) + 1; 
        int paramOutHeight  = param.out_shapes[0].Height;
        int paramOutWidth   = param.out_shapes[0].Width;
        if(param.padMode == PaddingType::PaddingSAME){
            outHeight       = (inHeight + k_sY - 1) / k_sY; // oh = ceil(ih / stride)
            outWidth        = (inWidth + k_sX - 1) / k_sX; // ow = ceil(iw / stride)
            int pad_width   = ARGSMAX(0, (outWidth - 1) * k_sX + DkernelX - inWidth);
            int pad_height  = ARGSMAX(0, (outHeight - 1) * k_sY + DkernelY - inHeight);
            k_padYT         = std::floor(pad_height / 2);
            k_padXL         = std::floor(pad_width / 2);
            k_padYB         = pad_height - k_padYT;
            k_padXR         = pad_width - k_padXL;
            outWidth        = std::floor((inWidth - DkernelX + pad_width) / k_sX) + 1;
            outHeight       = std::floor((inHeight - DkernelY + pad_height) / k_sY) + 1;
            
        }
        LOG_CHECK(outHeight == paramOutHeight)<< "outHeight: "<<outHeight<<", paramOutHeight: "<<paramOutHeight;
        LOG_CHECK(outWidth  == paramOutWidth)<< "outwidth: "<<outWidth<<", paramOutWidth: "<<paramOutWidth;
        //conv kernel param
        node.node_param.conv_strides        = {k_sY, k_sX};
        node.node_param.conv_paddingL       = {k_padYT, k_padXL};
        node.node_param.conv_paddingR       = {k_padYB, k_padXR};
        //primitive bottom shape & usr bottom shape
        node.node_param.bottom_shape        = {inBatch, inChannel, inHeight, inWidth};
        //primitive weights shape & usr weights shape
        node.node_param.groups              = param.groups >= 1 ? param.groups : 1;
        dnnl::memory::dims usr_weights_shape;
        if(node.node_param.groups > 1){
            LOG_CHECK(inChannel % node.node_param.groups == 0)<<inChannel<<","<<node.node_param.groups;
            LOG_CHECK(k_c % node.node_param.groups == 0)<<k_c<<","<<node.node_param.groups;
            int ICg             = inChannel / node.node_param.groups;
            int OCg             = k_c / node.node_param.groups;
            node.node_param.weights_shape   = {node.node_param.groups, OCg, ICg, k_h, k_w};
            usr_weights_shape               = {k_c, inChannel, k_h, k_w};
        }else if(node.node_param.groups == 1){
            node.node_param.weights_shape   = {k_c, inChannel, k_h, k_w};
            usr_weights_shape               = {k_c, inChannel, k_h, k_w};
        }
        // primitive bias shape
        node.node_param.hasBias             = param.hasBias;
        if(node.node_param.hasBias)
            node.node_param.bias_shape      = {k_c};
        
        // src bottom_md
        node.node_param.src_bottom_md       = memory::desc({node.node_param.bottom_shape}, dnnDataType, data_order);
        // weights & bias
        node.node_param.src_weights_md      = memory::desc({node.node_param.weights_shape}, dnnDataType, weights_order);
        if(node.node_param.groups > 1){
            node.node_param.src_weights_memory = memory({{node.node_param.weights_shape}, dnnDataType, weights_order}, BrixLab::graph_eng);
        }else if(node.node_param.groups == 1){
            node.node_param.src_weights_memory = memory({{node.node_param.weights_shape}, dnnDataType, weights_order}, BrixLab::graph_eng);
        }
        write_to_dnnl_memory(param.conv_weights, node.node_param.src_weights_memory);
        if(node.node_param.hasBias){
            node.node_param.src_bias_md = memory::desc({node.node_param.bias_shape}, biasDataType, tag::any);
            node.node_param.src_bias_memory = memory({{node.node_param.bias_shape}, biasDataType, tag::x}, BrixLab::graph_eng);
            if(quantized_type == QUANITIZED_TYPE::UINT8_QUANTIZED){
                write_to_dnnl_memory(param.quantized_bias, node.node_param.src_bias_memory);
            }else if(quantized_type == QUANITIZED_TYPE::FLOAT32_REGULAR){
                write_to_dnnl_memory(param.conv_bias, node.node_param.src_bias_memory);
            }
        }
        // output feature shape
        node.node_param.top_shape       = {inBatch, k_c, outHeight, outWidth};
        node.node_param.top_md    = memory::desc({node.node_param.top_shape}, dnnDataType, data_order);

        LOG_CHECK(!node.node_param.src_bottom_md.is_zero())<<"CHECK src_bottom_md";
        LOG_CHECK(!node.node_param.src_bias_md.is_zero())<<"CHECK src_bias_md";
        LOG_CHECK(!node.node_param.src_weights_md.is_zero())<<"CHECK src_weights_md";
        LOG_CHECK(!node.node_param.top_md.is_zero())<<"CHECK top_md";
        print_dnnl_memory_shape(node.node_param.bottom_shape, "bottom_shape");
        print_dnnl_memory_shape(node.node_param.top_shape, "top_shape");
            
        if(dilate){
            memory::dims conv_dilates = {node.node_param.dilateX, node.node_param.dilateX};
            if(node.node_param.hasBias){
                auto convolution_desc = convolution_forward::desc(prop_kind::forward_inference,
                                          algorithm::convolution_direct,node.node_param.src_bottom_md, node.node_param.src_weights_md,
                                          node.node_param.src_bias_md, node.node_param.top_md, node.node_param.conv_strides, conv_dilates,
                                          node.node_param.conv_paddingL, node.node_param.conv_paddingR);
                if(param.fused_ops){
                    node.node_param.conv_post_param = get_posts_opsMap(param.fused_act_type);
                    node.node_param.conv_ops.append_eltwise(node.node_param.conv_post_param.scale, 
                                                    node.node_param.conv_post_param.posts_op, 
                                                    node.node_param.conv_post_param.alpha,
                                                    node.node_param.conv_post_param.beta);
                    node.node_param.conv_attr.set_post_ops(node.node_param.conv_ops);
                    node.node_param.conv_pdesc = convolution_forward::primitive_desc(convolution_desc, node.node_param.conv_attr, BrixLab::graph_eng);
                }else{
                    node.node_param.conv_pdesc = convolution_forward::primitive_desc(convolution_desc, BrixLab::graph_eng);
                }
            }else{
                auto convolution_desc = convolution_forward::desc(prop_kind::forward_inference,
                                          algorithm::convolution_direct,node.node_param.src_bottom_md, node.node_param.src_weights_md,
                                          node.node_param.top_md, node.node_param.conv_strides, conv_dilates,
                                          node.node_param.conv_paddingL, node.node_param.conv_paddingR);
                if(param.fused_ops){
                    node.node_param.conv_post_param = get_posts_opsMap(param.fused_act_type);
                    node.node_param.conv_ops.append_eltwise(node.node_param.conv_post_param.scale, 
                                                    node.node_param.conv_post_param.posts_op, 
                                                    node.node_param.conv_post_param.alpha,
                                                    node.node_param.conv_post_param.beta);
                    node.node_param.conv_attr.set_post_ops(node.node_param.conv_ops);
                    node.node_param.conv_pdesc = convolution_forward::primitive_desc(convolution_desc, node.node_param.conv_attr, BrixLab::graph_eng);
                }else{
                    node.node_param.conv_pdesc = convolution_forward::primitive_desc(convolution_desc, BrixLab::graph_eng);
                }
            }    
        }else{
            if(node.node_param.hasBias){
                auto convolution_desc = convolution_forward::desc(prop_kind::forward_inference,
                                        algorithm::convolution_direct, node.node_param.src_bottom_md, node.node_param.src_weights_md,
                                        node.node_param.src_bias_md, node.node_param.top_md, node.node_param.conv_strides, 
                                        node.node_param.conv_paddingL, node.node_param.conv_paddingR);
                if(param.fused_ops){
                    node.node_param.conv_post_param = get_posts_opsMap(param.fused_act_type);
                    node.node_param.conv_ops.append_eltwise(node.node_param.conv_post_param.scale, 
                                                    node.node_param.conv_post_param.posts_op, 
                                                    node.node_param.conv_post_param.alpha,
                                                    node.node_param.conv_post_param.beta);
                    node.node_param.conv_attr.set_post_ops(node.node_param.conv_ops);
                    node.node_param.conv_pdesc = convolution_forward::primitive_desc(convolution_desc, node.node_param.conv_attr, BrixLab::graph_eng);
                }else{
                    node.node_param.conv_pdesc = convolution_forward::primitive_desc(convolution_desc, BrixLab::graph_eng);
                }
            }else{
                auto convolution_desc = convolution_forward::desc(prop_kind::forward_inference, 
                                        algorithm::convolution_direct, node.node_param.src_bottom_md, 
                                        node.node_param.src_weights_md, node.node_param.top_md, 
                                        node.node_param.conv_strides, node.node_param.conv_paddingL, node.node_param.conv_paddingR);
                if(param.fused_ops){
                    node.node_param.conv_post_param = get_posts_opsMap(param.fused_act_type);
                    node.node_param.conv_ops.append_eltwise(node.node_param.conv_post_param.scale, 
                                                    node.node_param.conv_post_param.posts_op, 
                                                    node.node_param.conv_post_param.alpha,
                                                    node.node_param.conv_post_param.beta);
                    node.node_param.conv_attr.set_post_ops(node.node_param.conv_ops);
                    node.node_param.conv_pdesc = convolution_forward::primitive_desc(convolution_desc, node.node_param.conv_attr, BrixLab::graph_eng);
                }else{
                    node.node_param.conv_pdesc = convolution_forward::primitive_desc(convolution_desc, BrixLab::graph_eng);
                }
            }
        }

        if (node.node_param.conv_pdesc.weights_desc() != node.node_param.src_weights_memory.get_desc()) {
            auto temp_memory = memory(node.node_param.conv_pdesc.weights_desc(), BrixLab::graph_eng);
            reorder(node.node_param.src_weights_memory, temp_memory)
                    .execute(BrixLab::graph_stream, node.node_param.src_weights_memory, temp_memory);
            node.node_param.src_weights_memory = temp_memory;
        }

        node.node_param.top_memory    = memory(node.node_param.conv_pdesc.dst_desc(), BrixLab::graph_eng);
        node.node_param.inference_forward   = OP_convolution_inference_forward; 
        LOG(DEBUG_INFO)<<"conv_set_up done!\n"<<std::endl;
        return node;
    }
    INSTANCE_LAYEROP(convolution);
    
    template<typename DType>
    void OP_batchnorm_inference_forward(strNodeParam<DType> *node, graphSetLink<DType> &g_net){
        node->src_bottom_memory = g_net[node->inputs[0]]->node_param.top_memory;;
        node->op_args           = {{DNNL_ARG_SRC, node->src_bottom_memory},
                                    {DNNL_ARG_MEAN, node->batchnorm_mean_memory},
                                    {DNNL_ARG_VARIANCE, node->batchnorm_variance_memory},
                                    {DNNL_ARG_SCALE_SHIFT, node->batchnorm_scale_shift_memory},
                                    {DNNL_ARG_DST, node->src_bottom_memory}};
        batch_normalization_forward(node->batchnorm_pdesc).execute(BrixLab::graph_stream, node->op_args);
        LOG(DEBUG_INFO)<<"batchnorm_inference done!\n"<<std::endl;
    }
        
    template<typename DType>
    strLayerNode<DType> OP_batchnorm_layer_setup(const strParam<DType> &param){
        strLayerNode<DType> node(strNodeParam<DType>(OP_type::BATCHNORM));
        LOG(DEBUG_INFO)<<get_quantized_type(param.quantized_type)<<",op: "<<get_mapped_op_string(param.op_type);
        node.node_param.node_name       = param.node_name;
        node.node_param.op_type         = param.op_type;
        QUANITIZED_TYPE quantized_type  = param.quantized_type;
        memory::data_type dnnDataType   = dt::f32;
        if(quantized_type == BrixLab::QUANITIZED_TYPE::UINT8_QUANTIZED){
            dnnDataType = dt::u8;
        }
        dnnl::memory::format_tag data_order       = tag::nchw;
        LOG_CHECK(param.in_shapes[0].format == param.out_shapes[0].format);

        node.node_param.inputs.resize(param.inIndexs.size());
        node.node_param.inputs          = param.inIndexs;
        node.node_param.outputs.resize(param.outIndexs.size());
        node.node_param.outputs         = param.outIndexs;

        LOG_CHECK(param.in_shapes.size() == 1)<<"CHECK_INPUTS ERROR";
        LOG_CHECK(param.out_shapes.size() == 1)<<"CHECK_OUTPUTS ERROR";
        node.node_param.in_shapes.resize(1);
        node.node_param.out_shapes.resize(1);
        node.node_param.in_shapes[0]    = param.in_shapes[0];
        node.node_param.out_shapes[0]   = param.out_shapes[0];

        int inHeight                    = param.in_shapes[0].Height;
        int inChannel                   = param.in_shapes[0].Channel;
        int inWidth                     = param.in_shapes[0].Width;
        int inBatch                     = param.in_shapes[0].Batch;

        node.node_param.bottom_shape   = {inBatch, inChannel, inHeight, inWidth};
        node.node_param.top_shape      = {inBatch, inChannel, inHeight, inWidth};
        
        node.node_param.batchnorm_scale_shift_shape = {2, inChannel};
        

        print_dnnl_memory_shape(node.node_param.bottom_shape, "bottom_shape");
        print_dnnl_memory_shape(node.node_param.top_shape, "top_shape");

        // src
        node.node_param.src_bottom_md                  = memory::desc(node.node_param.bottom_shape, dnnDataType, data_order);
        // scale_shift_weights
        node.node_param.batchnorm_scale_shift_md       = memory::desc(node.node_param.batchnorm_scale_shift_shape, dnnDataType, tag::nc);
        node.node_param.batchnorm_scale_shift_memory   = memory(node.node_param.batchnorm_scale_shift_md, BrixLab::graph_eng);
        write_to_dnnl_memory(param.b_shift_scale, node.node_param.batchnorm_scale_shift_memory);

        print_dnnl_memory_shape(node.node_param.bottom_shape, "bottom_shape");
        print_dnnl_memory_shape(node.node_param.top_shape, "top_shape");

        // Create operation descriptor.
        dnnl::batch_normalization_forward::desc batchnorm_desc = batch_normalization_forward::desc(
                                                                        prop_kind::forward_inference, node.node_param.src_bottom_md, 1.e-10f,
                                                                        normalization_flags::use_scale_shift
                                                                        | normalization_flags::use_global_stats);

        // Create primitive descriptor.
        node.node_param.batchnorm_pdesc             = batch_normalization_forward::primitive_desc(batchnorm_desc, BrixLab::graph_eng);
        // descriptor: mean, variance.
        node.node_param.batchnorm_mean_memory       = memory(node.node_param.batchnorm_pdesc.mean_desc(), BrixLab::graph_eng);
        node.node_param.batchnorm_variance_memory   = memory(node.node_param.batchnorm_pdesc.variance_desc(), BrixLab::graph_eng);
        write_to_dnnl_memory(param.b_means, node.node_param.batchnorm_mean_memory);
        write_to_dnnl_memory(param.b_variance, node.node_param.batchnorm_variance_memory);
        // top memory
        node.node_param.top_memory            = memory(node.node_param.batchnorm_pdesc.dst_desc(), BrixLab::graph_eng);
        node.node_param.inference_forward           = OP_batchnorm_inference_forward;
        LOG(DEBUG_INFO)<<"batchnorm_set_up done!\n"<<std::endl;
        return node;
    }
    INSTANCE_LAYEROP(batchnorm);

    template<typename DType>   
    void OP_pooling_inference_forward(strNodeParam<DType> *node, graphSetLink<DType> &g_net){
        node->src_bottom_memory = g_net[node->inputs[0]]->node_param.top_memory;;
        node->op_args           = {{DNNL_ARG_SRC, node->src_bottom_memory},
                                   {DNNL_ARG_DST, node->top_memory}};
        if(!node->p_dialiated){
            pooling_forward(node->pooling_pdesc_without_d).execute(BrixLab::graph_stream, node->op_args);
        }else if(node->p_dialiated){
            pooling_v2_forward(node->pooling_pdesc).execute(BrixLab::graph_stream, node->op_args);
        }
        #ifdef USE_DUMP_FILE
        float *dst  = (float*)node->top_memory.get_data_handle();
        size_t size = node->top_memory.get_desc().get_size();
        dump_data_to_file(dst, size / 4, node->node_name.c_str(), node->top_memory.get_desc().dims());
        #endif
        LOG(DEBUG_INFO)<<"pooling inference done!\n"<<std::endl;
    }
    template<typename DType>    
    strLayerNode<DType> OP_pooling_layer_setup(const strParam<DType> &param){
        strLayerNode<DType> node(strNodeParam<DType>(OP_type::POOLING));
        LOG(DEBUG_INFO)<<get_quantized_type(param.quantized_type)<<",op: "<<get_mapped_op_string(param.op_type);
        node.node_param.node_name           = param.node_name;
        node.node_param.op_type             = param.op_type;
        QUANITIZED_TYPE quantized_type      = param.quantized_type;
        memory::data_type dnnDataType       = dt::f32;
        if(quantized_type == BrixLab::QUANITIZED_TYPE::UINT8_QUANTIZED){
            dnnDataType = dt::u8;
        }
        dnnl::memory::format_tag data_order = tag::nchw;
        LOG_CHECK(param.in_shapes[0].format == param.out_shapes[0].format);
        
        LOG_CHECK(param.in_shapes.size() == 1)<<"CHECK INPUTS ERROR";
        LOG_CHECK(param.out_shapes.size() == 1)<<"CHECK INPUTS ERROR";
        node.node_param.in_shapes.resize(1);
        node.node_param.out_shapes.resize(1);
        node.node_param.in_shapes[0]   = param.in_shapes[0];
        node.node_param.out_shapes[0]  = param.out_shapes[0];
        node.node_param.inputs.resize(param.inIndexs.size());
        node.node_param.inputs              = param.inIndexs;
        node.node_param.outputs.resize(param.outIndexs.size());
        node.node_param.outputs             = param.outIndexs;
        int inHeight        = param.in_shapes[0].Height;
        int inChannel       = param.in_shapes[0].Channel;
        int inWidth         = param.in_shapes[0].Width;
        int inBatch         = param.in_shapes[0].Batch;
        int k_w             = param.p_kernelsX;
        int k_h             = param.p_kernelsY;
        int k_sX            = param.p_stridesX;
        int k_sY            = param.p_stridesY;
        int k_padYT         = 0;
        int k_padYB         = 0;
        int k_padXL         = 0;
        int k_padXR         = 0;
        int dilatedX        = param.p_dilatedX;
        int dilatedY        = param.p_dilatedY;
        int DkernelX        = 1 + (k_w - 1) * (dilatedX + 1);
        int DkernelY        = 1 + (k_h - 1) * (dilatedY + 1);
        bool dilated        = false;
        LOG_CHECK(dilatedX >= 0)<<"CHECK dilatedX";
        LOG_CHECK(dilatedY >= 0)<<"CHECK dilatedY";
        if(dilatedY == 0  && dilatedX == 0){
            dilated             = false;
        }else{
            dilated             = true;
        }
        PoolingType p_type                  = param.pooling_type;
        node.node_param.pooling_type        = get_op_mapped_pooling_type(p_type);
        node.node_param.bottom_shape        = {inBatch, inChannel, inHeight, inWidth};
        BrixLab::PaddingType pad_type       = param.pooling_padType;
        int outHeight = (inHeight - DkernelY + k_padYB + k_padYT) / k_sX + 1;
        int outWidth = (inWidth - DkernelX + k_padXL + k_padXR) / k_sY + 1;;
        if(pad_type == PaddingType::PaddingSAME){
            outHeight       = (inHeight + k_sY - 1) / k_sY; // oh = ceil(ih / stride)
            outWidth        = (inWidth + k_sX - 1) / k_sX; // ow = ceil(iw / stride)
            int pad_width   = ARGSMAX(0, (outWidth - 1) * k_sX + DkernelX - inWidth);
            int pad_height  = ARGSMAX(0, (outHeight - 1) * k_sY + DkernelY - inHeight);
            k_padYT         = std::floor(pad_height / 2);
            k_padXL         = std::floor(pad_width / 2);
            k_padYB         = pad_height - k_padYT;
            k_padXR         = pad_width - k_padXL;
            outWidth        = std::floor((inWidth - DkernelX + pad_width) / k_sX) + 1;
            outHeight       = std::floor((inHeight - DkernelY + pad_height) / k_sY) + 1;
            
        }
        int paramOutHeight  = param.out_shapes[0].Height;
        int paramOutWidth   = param.out_shapes[0].Width;
        LOG_CHECK(outHeight == paramOutHeight)<<"CHECK POOLING OUTHEIGHT ERROR: "<<outHeight <<", "<< paramOutHeight;
        LOG_CHECK(outWidth  == paramOutWidth)<<"CHECK POOLING OUTWIDTH ERROR: "<<outWidth  <<", "<< paramOutWidth;
        
        node.node_param.pooling_kernel      = {k_h, k_w};
        node.node_param.pooling_strides     = {k_sY, k_sX};
        node.node_param.pooling_paddingL    = {k_padYT, k_padXL};
        node.node_param.pooling_paddingR    = {k_padYB, k_padXR};
        node.node_param.pooling_dialiate    = {dilatedX, dilatedY};
        
        node.node_param.top_shape           = {inBatch, inChannel, outHeight, outWidth};
        node.node_param.top_md              = memory::desc(node.node_param.top_shape, dnnDataType, data_order);
        node.node_param.src_bottom_md       = memory::desc(node.node_param.bottom_shape, dnnDataType, data_order);

        print_dnnl_memory_shape(node.node_param.bottom_shape, "bottom_shape");
        print_dnnl_memory_shape(node.node_param.top_shape, "top_shape");
        if(dilated){
            dnnl::pooling_v2_forward::desc pooling_desc = pooling_v2_forward::desc(prop_kind::forward_inference,
                                            node.node_param.pooling_type, node.node_param.src_bottom_md, 
                                            node.node_param.top_md,
                                            node.node_param.pooling_strides, node.node_param.pooling_kernel,
                                            node.node_param.pooling_dialiate, node.node_param.pooling_paddingL, 
                                            node.node_param.pooling_paddingR);
            node.node_param.pooling_pdesc   = pooling_v2_forward::primitive_desc(pooling_desc, BrixLab::graph_eng);
            node.node_param.top_memory      = memory(node.node_param.pooling_pdesc.dst_desc(), BrixLab::graph_eng);
        }else if(!dilated){
            dnnl::pooling_forward::desc pooling_desc_without_d = pooling_forward::desc(prop_kind::forward_inference, 
                                                node.node_param.pooling_type, node.node_param.src_bottom_md, 
                                                node.node_param.top_md, node.node_param.pooling_strides, 
                                                node.node_param.pooling_kernel,
                                                node.node_param.pooling_paddingL,node.node_param.pooling_paddingR);
            node.node_param.pooling_pdesc_without_d = pooling_forward::primitive_desc(pooling_desc_without_d, BrixLab::graph_eng);
            node.node_param.top_memory        = memory(node.node_param.pooling_pdesc_without_d.dst_desc(), BrixLab::graph_eng);
        }
        node.node_param.inference_forward           = OP_pooling_inference_forward;
        LOG(DEBUG_INFO)<<"pool_set_up done!\n"<<std::endl;
        return node;
    }
    INSTANCE_LAYEROP(pooling);

    template<typename DType>     
    void OP_concat_inference_forward(strNodeParam<DType> *node, graphSetLink<DType> &g_net){
        for(int ii = 0; ii < node->concat_num; ii++){
            int index                        = node->inputs[ii];
            node->concat_bottom_md[ii]       = g_net[index]->node_param.top_md;
            node->concat_bottom_memory[ii]   = g_net[index]->node_param.top_memory;
        }
        if(!node->inputset){
            for(int ii = 0; ii < node->concat_num; ii++){
                node->op_args.insert({DNNL_ARG_MULTIPLE_SRC + ii, node->concat_bottom_memory[ii]});
            }
            node->op_args.insert({{DNNL_ARG_DST, node->top_memory}});
            node->inputset = true;
        }else{
            for(int ii = 0; ii < node->concat_num; ii++){
                node->op_args[DNNL_ARG_MULTIPLE_SRC + ii] = node->concat_bottom_memory[ii];
            }
            node->op_args[DNNL_ARG_DST] = node->top_memory;
        }
        concat(node->concat_pdesc).execute(BrixLab::graph_stream, node->op_args);
        #ifdef USE_DUMP_FILE
        float *dst  = (float*)node->top_memory.get_data_handle();
        size_t size = node->top_memory.get_desc().get_size();
        dump_data_to_file(dst, size / 4, node->node_name.c_str(), node->top_memory.get_desc().dims());
        #endif
        LOG(DEBUG_INFO)<<"concat inference done!\n"<<std::endl;
    }
    template<typename DType>
    strLayerNode<DType> OP_concat_layer_setup(const strParam<DType> &param){
        strLayerNode<DType> node(strNodeParam<DType>(OP_type::CONCAT));
        LOG(DEBUG_INFO)<<get_quantized_type(param.quantized_type)<<",op: "<<get_mapped_op_string(param.op_type);
        node.node_param.node_name       = param.node_name;
        node.node_param.op_type         = param.op_type;
        QUANITIZED_TYPE quantized_type  = param.quantized_type;
        memory::data_type dnnDataType   = dt::f32;
        if(quantized_type == BrixLab::QUANITIZED_TYPE::UINT8_QUANTIZED){
            dnnDataType = dt::u8;
        }
        dnnl::memory::format_tag data_order = tag::nchw;
        LOG_CHECK(param.in_shapes[0].format == param.out_shapes[0].format);
        check_inputs_shape(param.in_shapes);
        if(param.in_shapes[0].format == TENSOR_FORMATE::NHWC){
            node.node_param.concat_axis = param.concat_axis - 1;
        }else{
            node.node_param.concat_axis = 1 ;
        }
        node.node_param.in_shapes.resize(node.node_param.concat_num);
        for(int ii = 0; ii < node.node_param.concat_num; ii++){
            node.node_param.in_shapes[ii] = param.in_shapes[ii];
        }
        node.node_param.out_shapes.resize(1);
        node.node_param.out_shapes[0]   = param.out_shapes[0];
        node.node_param.outputs.resize(1);
        node.node_param.outputs[0]      = param.outIndexs[0];
        int inHeight                    = param.in_shapes[0].Height;
        int inChannel                   = param.in_shapes[0].Channel;
        int inWidth                     = param.in_shapes[0].Width;
        int inBatch                     = param.in_shapes[0].Batch;
        node.node_param.op_type         = param.op_type;
        node.node_param.concat_num     = param.inIndexs.size();
        node.node_param.concat_axis    = 1 ;

        node.node_param.bottom_shape   = {inBatch, inChannel, inHeight, inWidth};
        
        node.node_param.src_bottom_md  = memory::desc(node.node_param.bottom_shape, dnnDataType, data_order);
        for(int ii = 0; ii < node.node_param.concat_num; ii++){
            node.node_param.inputs.push_back(param.inIndexs[ii]);
            node.node_param.concat_bottom_md.push_back(node.node_param.src_bottom_md);
        }      

        node.node_param.concat_bottom_memory.resize(node.node_param.concat_num);
        node.node_param.concat_pdesc       = concat::primitive_desc(node.node_param.concat_axis, node.node_param.concat_bottom_md, 
                                                                        BrixLab::graph_eng);
        node.node_param.top_md       = node.node_param.concat_pdesc.dst_desc();
        node.node_param.top_shape          = node.node_param.top_md.dims();
        print_dnnl_memory_shape(node.node_param.bottom_shape, "bottom_shape");
        print_dnnl_memory_shape(node.node_param.top_shape, "top_shape");
        node.node_param.top_memory   = memory(node.node_param.concat_pdesc.dst_desc(), BrixLab::graph_eng);
        node.node_param.inference_forward  = OP_concat_inference_forward;
        LOG(DEBUG_INFO)<<"concat_set_up done!\n"<<std::endl;
        return node;
    }
    INSTANCE_LAYEROP(concat);

    template<typename DType>    
    void OP_sum_inference_forward(strNodeParam<DType> *node, graphSetLink<DType> &g_net){
        for(int ii = 0; ii < node->sum_num; ii++){
            int index           = node->inputs[ii];
            node->sum_bottom_memory[ii] = g_net[index]->node_param.top_memory;
        }
        if(!node->inputset){
            for(int ii = 0; ii < node->sum_num; ii++){
                node->op_args.insert({DNNL_ARG_MULTIPLE_SRC + ii, node->sum_bottom_memory[ii]});
            }
            node->op_args.insert({DNNL_ARG_DST, node->top_memory});
        }else{
            for(int ii = 0; ii < node->sum_num; ii++){
                node->op_args[DNNL_ARG_MULTIPLE_SRC + ii] = node->sum_bottom_memory[ii];
            }
            node->op_args[DNNL_ARG_DST]= node->top_memory;
        }
        sum(node->sum_pdesc).execute(BrixLab::graph_stream, node->op_args);
        LOG(DEBUG_INFO)<<"sum inference done!\n"<<std::endl;
    }

    template<typename DType>    
    strLayerNode<DType> OP_sum_layer_setup(const strParam<DType> &param){
        strLayerNode<DType> node(strNodeParam<DType>(OP_type::ELTWISE));
        LOG(DEBUG_INFO)<<get_quantized_type(param.quantized_type)<<",op: "<<get_mapped_op_string(param.op_type);
        node.node_param.node_name       = param.node_name;
        node.node_param.op_type         = param.op_type;
        QUANITIZED_TYPE quantized_type  = param.quantized_type;
        memory::data_type dnnDataType   = dt::f32;
        if(quantized_type == BrixLab::QUANITIZED_TYPE::UINT8_QUANTIZED){
            dnnDataType = dt::u8;
        }
        dnnl::memory::format_tag data_order = tag::nchw;
        LOG_CHECK(param.in_shapes[0].format == param.out_shapes[0].format);
        check_inputs_shape(param.in_shapes);
        node.node_param.sum_num = param.inIndexs.size();
        for(int ii = 0; ii < node.node_param.sum_num; ii++){
            node.node_param.inputs.push_back(param.inIndexs[ii]);
        }
        node.node_param.out_shapes.resize(1);
        node.node_param.out_shapes[0]  = param.out_shapes[0];
        node.node_param.in_shapes.resize(node.node_param.sum_num);
        for(int ii = 0; ii < node.node_param.sum_num; ii++){
            node.node_param.in_shapes[ii] = param.in_shapes[ii];
        }
        int inHeight    = param.in_shapes[0].Height;
        int inChannel   = param.in_shapes[0].Channel;
        int inWidth     = param.in_shapes[0].Width;
        int inBatch     = param.in_shapes[0].Batch;
        node.node_param.bottom_shape    = {inBatch, inChannel, inWidth, inHeight};
        node.node_param.src_bottom_md   = memory::desc(node.node_param.bottom_shape, dnnDataType, data_order);
        for(int ii = 0; ii < node.node_param.sum_num; ii++){
            node.node_param.sum_scale[ii] = 1.f;
            node.node_param.sum_bottom_md.push_back(node.node_param.src_bottom_md);
        }
        
        node.node_param.sum_bottom_memory.resize(node.node_param.sum_num);
        node.node_param.sum_pdesc       = sum::primitive_desc(node.node_param.sum_scale, node.node_param.sum_bottom_md, BrixLab::graph_eng);
        node.node_param.top_md          = node.node_param.sum_pdesc.dst_desc();
        node.node_param.top_memory      = memory(node.node_param.sum_pdesc.dst_desc(), BrixLab::graph_eng);

        print_dnnl_memory_shape(node.node_param.bottom_shape, "bottom_shape");
        print_dnnl_memory_shape(node.node_param.top_shape, "top_shape");
        
        for(int ii = 0; ii < node.node_param.sum_num; ii++){
            node.node_param.op_args.insert({DNNL_ARG_MULTIPLE_SRC + ii, node.node_param.sum_bottom_memory[ii]});
        }
        node.node_param.inference_forward = OP_sum_inference_forward;
        LOG(DEBUG_INFO)<<"sum_set_up done!\n"<<std::endl;
        return node;
    }
    INSTANCE_LAYEROP(sum);
    
    template<typename DType>
    void OP_resample_inference_forward(strNodeParam<DType> *node, graphSetLink<DType> &g_net){
        node->src_bottom_memory = g_net[node->inputs[0]]->node_param.top_memory;;
        node->op_args           = {{DNNL_ARG_SRC, node->src_bottom_memory},
                                    {DNNL_ARG_DST, node->top_memory}};
        resampling_forward(node->resample_pdesc).execute(BrixLab::graph_stream, node->op_args);
        #ifdef USE_DUMP_FILE
        float *dst  = (float*)node->top_memory.get_data_handle();
        size_t size = node->top_memory.get_desc().get_size();
        dump_data_to_file(dst, size / 4, node->node_name.c_str(), node->top_memory.get_desc().dims());
        #endif
        LOG(DEBUG_INFO)<<"resample inference done!\n"<<std::endl;
    }
    template<typename DType>
    strLayerNode<DType> OP_resample_layer_setup(const strParam<DType> &param){
        strLayerNode<DType> node(strNodeParam<DType>(OP_type::RESAMPLING));
        LOG(DEBUG_INFO)<<get_quantized_type(param.quantized_type)<<",op: "<<get_mapped_op_string(param.op_type);
        node.node_param.node_name       = param.node_name;
        node.node_param.op_type         = param.op_type;
        QUANITIZED_TYPE quantized_type  = param.quantized_type;
        memory::data_type dnnDataType   = dt::f32;
        if(quantized_type == BrixLab::QUANITIZED_TYPE::UINT8_QUANTIZED){
            dnnDataType = dt::u8;
        }
        dnnl::memory::format_tag data_order = tag::nchw;
        LOG_CHECK(param.in_shapes[0].format == param.out_shapes[0].format);
        LOG_CHECK(param.in_shapes.size()==1)<<"CHECK_INPUTS EEROR";
        LOG_CHECK(param.out_shapes.size()==1)<<"CHECK_OUTPUTS ERROR";
        node.node_param.inputs.resize(param.inIndexs.size());
        node.node_param.inputs              = param.inIndexs;
        node.node_param.outputs.resize(param.outIndexs.size());
        node.node_param.outputs             = param.outIndexs;
        node.node_param.in_shapes.resize(1);
        node.node_param.out_shapes.resize(1);
        node.node_param.in_shapes[0]        = param.in_shapes[0];
        node.node_param.out_shapes[0]       = param.out_shapes[0];
        int inHeight                        = param.in_shapes[0].Height;
        int inChannel                       = param.in_shapes[0].Channel;
        int inWidth                         = param.in_shapes[0].Width;
        int inBatch                         = param.in_shapes[0].Batch;
        int outHeight                       = param.out_shapes[0].Height;
        int outChannel                      = param.out_shapes[0].Channel;
        int outWidth                        = param.out_shapes[0].Width;
        int outBatch                        = param.out_shapes[0].Batch;
        node.node_param.bottom_shape        = {inBatch, inChannel, inHeight, inWidth};
        node.node_param.src_bottom_md       = memory::desc(node.node_param.bottom_shape, dnnDataType, data_order);
        node.node_param.top_shape           = {outBatch, outChannel, outHeight, outWidth};
        node.node_param.top_md        = memory::desc(node.node_param.top_shape, dnnDataType, data_order);

        dnnl::resampling_forward::desc resample_desc    = resampling_forward::desc(prop_kind::forward_inference,
                                                                                algorithm::resampling_linear, 
                                                                                node.node_param.src_bottom_md, 
                                                                                node.node_param.top_md);
        node.node_param.resample_pdesc                  = resampling_forward::primitive_desc(resample_desc, BrixLab::graph_eng);
        
        node.node_param.top_memory                = memory(node.node_param.resample_pdesc.dst_desc(), BrixLab::graph_eng);

        print_dnnl_memory_shape(node.node_param.bottom_shape, "bottom_shape");
        print_dnnl_memory_shape(node.node_param.top_shape, "top_shape");

        node.node_param.inference_forward = OP_resample_inference_forward;
        LOG(DEBUG_INFO)<<"resample_set_up done!\n"<<std::endl;
        return node;
    }
    INSTANCE_LAYEROP(resample);
    
    template<typename DType>
    void OP_deconvolution_inference_forward(strNodeParam<DType> *node, graphSetLink<DType> &g_net){
        auto temp_memory            = memory(g_net[node->inputs[0]]->node_param.top_memory.get_desc(), BrixLab::graph_eng);
        node->src_bottom_memory     = memory(g_net[node->inputs[0]]->node_param.top_memory);
        if (node->deconv_pdesc.src_desc() != temp_memory.get_desc()) {
            float* src_data         = (float*)g_net[node->inputs[0]]->node_param.top_memory.get_data_handle();
            write_to_dnnl_memory((void*)src_data, temp_memory);
            node->src_bottom_memory = memory(node->deconv_pdesc.src_desc(), BrixLab::graph_eng);
            std::unordered_map<int, memory> op_arg  = {{DNNL_ARG_FROM, temp_memory},
                                                        {DNNL_ARG_TO, node->src_bottom_memory}};
            reorder(temp_memory, node->src_bottom_memory).execute(BrixLab::graph_stream, op_arg);
        }
        if(node->hasBias){
            node->op_args = {{DNNL_ARG_SRC, node->src_bottom_memory},
                            {DNNL_ARG_WEIGHTS, node->src_weights_memory},
                            {DNNL_ARG_BIAS, node->src_bias_memory},
                            {DNNL_ARG_DST, node->top_memory}};
        }else{
            node->op_args = {{DNNL_ARG_SRC, node->src_bottom_memory},
                            {DNNL_ARG_WEIGHTS, node->src_weights_memory},
                            {DNNL_ARG_DST, node->top_memory}};
        }
        deconvolution_forward(node->deconv_pdesc).execute(BrixLab::graph_stream, node->op_args);
        LOG(DEBUG_INFO)<<"deconv_inference done!\n"<<std::endl;
    }    

    template<typename DType>
    strLayerNode<DType> OP_deconvolution_layer_setup(const strParam<DType> &param){
        strLayerNode<DType> node(strNodeParam<DType>(OP_type::DECONVOLUTION));
        LOG(DEBUG_INFO)<<get_quantized_type(param.quantized_type)<<",op: "<<get_mapped_op_string(param.op_type);
        node.node_param.node_name           = param.node_name;
        node.node_param.op_type             = param.op_type;
        QUANITIZED_TYPE quantized_type      = param.quantized_type;
        memory::data_type dnnDataType       = dt::f32;
        memory::data_type biasDataType   = dt::f32;
        if(quantized_type == BrixLab::QUANITIZED_TYPE::UINT8_QUANTIZED){
            dnnDataType = dt::u8;
            biasDataType = dt::s32;
        }
        dnnl::memory::format_tag data_order       = tag::nchw;
        dnnl::memory::format_tag weights_order       = tag::oihw;
        LOG_CHECK(param.in_shapes[0].format == param.out_shapes[0].format)<<"in_shapes[0].format == =out_shapes[0].format";
        
        node.node_param.inputs.resize(param.inIndexs.size());
        node.node_param.inputs              = param.inIndexs;
        node.node_param.outputs.resize(param.outIndexs.size());
        node.node_param.outputs             = param.outIndexs;
        
        LOG_CHECK(param.in_shapes.size()==1)<<"CHECK_INPUTS";
        LOG_CHECK(param.out_shapes.size()==1)<<"CHECK_OUTPUTS";
        node.node_param.in_shapes.resize(1);
        node.node_param.out_shapes.resize(1);
        node.node_param.in_shapes[0]   = param.in_shapes[0];
        node.node_param.out_shapes[0]  = param.out_shapes[0];
        int inHeight    = param.in_shapes[0].Height;
        int inChannel   = param.in_shapes[0].Channel;
        int inWidth     = param.in_shapes[0].Width;
        int inBatch     = param.in_shapes[0].Batch;
        int k_w         = param.k_w;
        int k_h         = param.k_h;
        int k_c         = param.k_c;
        int k_sX        = param.stridesX;
        int k_sY        = param.stridesY;
        int k_padXL     = 0;
        int k_padXR     = 0;
        int k_padYT     = 0;
        int k_padYB     = 0;
        node.node_param.dilateX     = param.dilateX;
        node.node_param.dilateY     = param.dilateY;
        int D_kHeight               = 1 + (k_h - 1) * (node.node_param.dilateX + 1);
        int D_kWidth                = 1 + (k_w - 1) * (node.node_param.dilateY + 1);
        int outWidth                = int((inWidth - 1) * k_sX + D_kWidth - (k_padXL + k_padXR));
        int outHeight               = int((inHeight - 1) * k_sY + D_kHeight - (k_padYT + k_padYB));
        int paramOutHeight          = param.out_shapes[0].Height;
        int paramOutWidth           = param.out_shapes[0].Width;
        if(param.padMode == PaddingType::PaddingSAME){
            outHeight       = inHeight * k_sY + 1 - k_sY;
            outWidth        = inWidth * k_sX + 1 - k_sX;
            int pad_width   = ARGSMAX(0, (inWidth - 1) * k_sX + D_kWidth - outWidth);
            int pad_height  = ARGSMAX(0, (inHeight - 1) * k_sY + D_kHeight - outHeight);
            k_padYT         = std::floor(pad_height / 2);
            k_padXL         = std::floor(pad_width / 2);
            k_padYB         = pad_height - k_padYT;
            k_padXR         = pad_width - k_padXL;
            outWidth        = int((inWidth - 1) * k_sX + D_kWidth - (k_padXL + k_padXR));
            outHeight       = int((inHeight - 1) * k_sY + D_kHeight - (k_padYT + k_padYB));
            
        }
        LOG_CHECK(outHeight == paramOutHeight)<<"CHECK PADDING TRANSPOSED_CONV OUTHEIGHT";
        LOG_CHECK(outWidth  == paramOutWidth)<<"CHECK PADDING TRANSPOSED_CONV OUTWIDTH";

        node.node_param.deconv_strides     = {k_sX, k_sY};
        node.node_param.deconv_paddingL    = {k_padYT, k_padXL};
        node.node_param.deconv_paddingR    = {k_padYB, k_padXR};

        node.node_param.bottom_shape            = {inBatch, inChannel, inHeight, inWidth};
        node.node_param.weights_shape           = {k_c, inChannel, k_h, k_w};
        node.node_param.top_shape               = {inBatch, k_c, outHeight, outWidth};
        node.node_param.hasBias                 = param.hasBias;
        if(node.node_param.hasBias)
            node.node_param.bias_shape = {k_c};
        // src bottom data
        node.node_param.src_bottom_md       = memory::desc({node.node_param.bottom_shape}, dnnDataType, data_order);
        node.node_param.top_md        = memory::desc({node.node_param.top_shape}, dnnDataType, data_order);
        // weights & bias
        node.node_param.src_weights_md      = memory::desc({node.node_param.weights_shape}, dnnDataType, weights_order);
        node.node_param.src_weights_memory  = memory({{node.node_param.weights_shape}, dnnDataType, weights_order}, BrixLab::graph_eng);
        write_to_dnnl_memory(param.transposed_weights, node.node_param.src_weights_memory);
        if(node.node_param.hasBias){
            node.node_param.src_bias_md     = memory::desc({node.node_param.bias_shape}, biasDataType, tag::any);
            node.node_param.src_bias_memory = memory({{node.node_param.bias_shape}, biasDataType, tag::x}, BrixLab::graph_eng);
            if(quantized_type == QUANITIZED_TYPE::UINT8_QUANTIZED){
                write_to_dnnl_memory(param.quantized_bias, node.node_param.src_bias_memory);
            }else if(quantized_type == QUANITIZED_TYPE::FLOAT32_REGULAR){
                write_to_dnnl_memory(param.transposed_bias, node.node_param.src_bias_memory);
            }
        }

        print_dnnl_memory_shape(node.node_param.bottom_shape, "bottom_shape");
        print_dnnl_memory_shape(node.node_param.top_shape, "top_shape");

        bool dilate = false;
        LOG_CHECK(node.node_param.dilateY >= 0)<<"CHECK ERROR";
        LOG_CHECK(node.node_param.dilateY >= 0)<<"CHECK ERROR";
        if(node.node_param.dilateX == 0 && node.node_param.dilateY == 0){
            dilate = false;
        }else{
            dilate = true;
        }
        if(dilate){
            memory::dims deconv_deliatd = {node.node_param.dilateX, node.node_param.dilateY};
            if(node.node_param.hasBias){
                auto deconv_desc  = deconvolution_forward::desc(prop_kind::forward_inference, algorithm::deconvolution_direct,
                                            node.node_param.src_bottom_md, node.node_param.src_weights_md,
                                            node.node_param.src_bias_md, node.node_param.top_md,
                                            node.node_param.deconv_strides, deconv_deliatd,
                                            node.node_param.deconv_paddingL,node.node_param.deconv_paddingR);
                node.node_param.deconv_pdesc = deconvolution_forward::primitive_desc(deconv_desc, BrixLab::graph_eng);

            }else{
                auto deconv_desc  = deconvolution_forward::desc(prop_kind::forward_inference, algorithm::deconvolution_direct,
                                            node.node_param.src_bottom_md, node.node_param.src_weights_md,
                                            node.node_param.top_md,
                                            node.node_param.deconv_strides, deconv_deliatd,
                                            node.node_param.deconv_paddingL, node.node_param.deconv_paddingR);
                node.node_param.deconv_pdesc = deconvolution_forward::primitive_desc(deconv_desc, BrixLab::graph_eng);
            }
        }else{
            if(node.node_param.hasBias){
                auto deconv_desc  = deconvolution_forward::desc(prop_kind::forward_inference, algorithm::deconvolution_direct,
                                            node.node_param.src_bottom_md, node.node_param.src_weights_md,
                                            node.node_param.src_bias_md, node.node_param.top_md,
                                            node.node_param.deconv_strides, node.node_param.deconv_paddingL,
                                            node.node_param.deconv_paddingR);
                node.node_param.deconv_pdesc = deconvolution_forward::primitive_desc(deconv_desc, BrixLab::graph_eng);
            }else{
                auto deconv_desc  = deconvolution_forward::desc(prop_kind::forward_inference, algorithm::deconvolution_direct,
                                            node.node_param.src_bottom_md, node.node_param.src_weights_md,
                                            node.node_param.top_md,
                                            node.node_param.deconv_strides, node.node_param.deconv_paddingL,
                                            node.node_param.deconv_paddingR);
                node.node_param.deconv_pdesc = deconvolution_forward::primitive_desc(deconv_desc, BrixLab::graph_eng);
            }
        }

        if (node.node_param.deconv_pdesc.weights_desc() != node.node_param.src_weights_memory.get_desc()) {
            auto temp_memory = memory(node.node_param.deconv_pdesc.weights_desc(), BrixLab::graph_eng);
            print_dnnl_memory_shape(temp_memory.get_desc().dims(), "weights shape");
            print_dnnl_memory_shape(node.node_param.src_weights_memory.get_desc().dims(), "src weights shape");
            reorder(node.node_param.src_weights_memory, temp_memory)
                    .execute(BrixLab::graph_stream, node.node_param.src_weights_memory, temp_memory);
            node.node_param.src_weights_memory = temp_memory;
        }
       
        node.node_param.top_memory = memory(node.node_param.deconv_pdesc.dst_desc(), BrixLab::graph_eng);
        
        node.node_param.inference_forward = OP_deconvolution_inference_forward;
        LOG(DEBUG_INFO)<<"deconv_set_up done!\n"<<std::endl;
        return node;
    }
    INSTANCE_LAYEROP(deconvolution);
    
    template<typename DType>
    void OP_innerproduct_inference_forward(strNodeParam<DType> *node, graphSetLink<DType> &g_net){
        node->src_bottom_memory = g_net[node->inputs[0]]->node_param.top_memory;
        node->op_args           = {{DNNL_ARG_SRC, node->src_bottom_memory},
                                    {DNNL_ARG_WEIGHTS, node->src_weights_memory},
                                    {DNNL_ARG_BIAS, node->src_bias_memory},
                                    {DNNL_ARG_DST, node->top_memory}};
        inner_product_forward(node->inner_pdesc).execute(BrixLab::graph_stream, node->op_args);
        LOG(DEBUG_INFO)<<"inner_product_inference done!\n"<<std::endl;
    }
   
    template<typename DType>
    strLayerNode<DType> OP_innerproduct_layer_setup(const strParam<DType> &param){
        strLayerNode<DType> node(strNodeParam<DType>(OP_type::INNERPRODUCT));
        LOG(DEBUG_INFO)<<get_quantized_type(param.quantized_type)<<",op: "<<get_mapped_op_string(param.op_type);
        node.node_param.node_name       = param.node_name;
        node.node_param.op_type         = param.op_type;
        QUANITIZED_TYPE quantized_type  = param.quantized_type;
        memory::data_type dnnDataType   = dt::f32;
        memory::data_type biasDataType  = dt::f32;
        if(quantized_type == BrixLab::QUANITIZED_TYPE::UINT8_QUANTIZED){
            dnnDataType     = dt::u8;
            biasDataType    = dt::s32;
        }
        dnnl::memory::format_tag data_order     = tag::nchw;
        dnnl::memory::format_tag weights_order       = tag::oihw;

        LOG_CHECK(param.in_shapes[0].format == param.out_shapes[0].format);
        node.node_param.inputs.resize(param.inIndexs.size());
        node.node_param.inputs              = param.inIndexs;
        node.node_param.outputs.resize(param.outIndexs.size());
        node.node_param.outputs             = param.outIndexs;

        LOG_CHECK(param.in_shapes.size() ==1)<<"CHECK_INPUTS ERROR";
        LOG_CHECK(param.out_shapes.size()==1)<<"CHECK_OUTPUTS ERROR";
        node.node_param.in_shapes.resize(1);
        node.node_param.out_shapes.resize(1);
        node.node_param.in_shapes[0]   = param.in_shapes[0];
        node.node_param.out_shapes[0]  = param.out_shapes[0];
        int k_c                         = param.k_c;
        int inHeight                    = param.in_shapes[0].Height;
        int inChannel                   = param.in_shapes[0].Channel;
        int inWidth                     = param.in_shapes[0].Width;
        int inBatch                     = param.in_shapes[0].Batch;
        
        node.node_param.bottom_shape            = {inBatch, inChannel, inHeight, inWidth};
        node.node_param.weights_shape           = {k_c, inChannel, inHeight, inWidth};
        node.node_param.bias_shape              = {k_c};
        
        // src bottom data
        node.node_param.src_bottom_md       = memory::desc({node.node_param.bottom_shape}, dnnDataType, data_order);
        // weights & bias
        node.node_param.src_weights_md      = memory::desc({node.node_param.weights_shape}, dnnDataType, weights_order);
        node.node_param.src_weights_memory  =  memory({node.node_param.weights_shape, dnnDataType, weights_order}, BrixLab::graph_eng);
        write_to_dnnl_memory(param.innerWeights, node.node_param.src_weights_memory);
        node.node_param.src_bias_md         = memory::desc({node.node_param.bias_shape}, biasDataType, tag::any);
        node.node_param.src_bias_memory     = memory({{node.node_param.bias_shape}, biasDataType, tag::x}, BrixLab::graph_eng);

        if(quantized_type == QUANITIZED_TYPE::UINT8_QUANTIZED){
            write_to_dnnl_memory(param.quantized_bias, node.node_param.src_bias_memory);
        }else if(quantized_type == QUANITIZED_TYPE::FLOAT32_REGULAR){
            write_to_dnnl_memory(param.innerBias, node.node_param.src_bias_memory);
        }

        node.node_param.top_shape = {inBatch, k_c};

        print_dnnl_memory_shape(node.node_param.bottom_shape, "bottom_shape");
        print_dnnl_memory_shape(node.node_param.top_shape, "top_shape");
        
        node.node_param.top_md = memory::desc({node.node_param.top_shape}, dnnDataType, tag::any);

        dnnl::inner_product_forward::desc inner_desc = inner_product_forward::desc(prop_kind::forward_inference,
                                                                                 node.node_param.src_bottom_md, 
                                                                                 node.node_param.src_weights_md, 
                                                                                 node.node_param.src_bias_md, 
                                                                                 node.node_param.top_md);
        
        if(param.fused_ops){
            node.node_param.fc_post_op = get_posts_opsMap(param.fused_act_type);
            node.node_param.fc_ops.append_eltwise(node.node_param.fc_post_op.scale, node.node_param.fc_post_op.posts_op, 
                                            node.node_param.fc_post_op.alpha, node.node_param.fc_post_op.beta);
            node.node_param.fc_attr.set_post_ops(node.node_param.fc_ops);
            node.node_param.inner_pdesc = inner_product_forward::primitive_desc(
                                                inner_desc, node.node_param.fc_attr, BrixLab::graph_eng);
        }else{
            node.node_param.inner_pdesc = inner_product_forward::primitive_desc(inner_desc, BrixLab::graph_eng);
        }

        if (node.node_param.inner_pdesc.weights_desc() != node.node_param.src_weights_memory.get_desc()) {
            auto temp_memory = memory(node.node_param.inner_pdesc.weights_desc(), BrixLab::graph_eng);
            reorder(node.node_param.src_weights_memory, temp_memory).execute(BrixLab::graph_stream, 
                                    node.node_param.src_weights_memory, temp_memory);
            node.node_param.src_weights_memory = temp_memory;
        }
        node.node_param.top_memory = memory(node.node_param.inner_pdesc.dst_desc(), BrixLab::graph_eng);

        node.node_param.inference_forward = OP_innerproduct_inference_forward;
        LOG(DEBUG_INFO)<<"inner_product_setup done!\n"<<std::endl;
        return node;
    }
    INSTANCE_LAYEROP(innerproduct);

    template<typename DType>
    void OP_activation_inference_forward(strNodeParam<DType> *node, graphSetLink<DType> &g_net){
        node->src_bottom_memory = g_net[node->inputs[0]]->node_param.top_memory;
        node->op_args = {
            {DNNL_ARG_SRC, node->src_bottom_memory},
            {DNNL_ARG_DST, node->top_memory}
        };
        eltwise_forward(node->eltwise_pdesc).execute(BrixLab::graph_stream, node->op_args);
        LOG(DEBUG_INFO)<<"eltwise_inference done!\n"<<std::endl;
    }
    template<typename DType>
    strLayerNode<DType> OP_activation_layer_setup(const strParam<DType> &param){
        strLayerNode<DType> node(strNodeParam<DType>(OP_type::ACTIVITION));
        LOG(DEBUG_INFO)<<get_quantized_type(param.quantized_type)<<",op: "<<get_mapped_op_string(param.op_type);
        node.node_param.node_name       = param.node_name;
        node.node_param.op_type         = param.op_type;
        QUANITIZED_TYPE quantized_type  = param.quantized_type;
        memory::data_type dnnDataType   = dt::f32;
        if(quantized_type == BrixLab::QUANITIZED_TYPE::UINT8_QUANTIZED){
            dnnDataType = dt::u8;
        }
        dnnl::memory::format_tag data_order     = tag::nchw;
        LOG_CHECK(param.in_shapes[0].format == param.out_shapes[0].format);
        node.node_param.inputs.resize(param.inIndexs.size());
        node.node_param.inputs              = param.inIndexs;
        node.node_param.outputs.resize(param.outIndexs.size());
        node.node_param.outputs             = param.outIndexs;
        LOG_CHECK(param.in_shapes.size()==1)<<"CHECK_INPUTS";
        LOG_CHECK(param.out_shapes.size()==1)<<"CHECK_OUTPUTS";
        node.node_param.in_shapes.resize(1);
        node.node_param.out_shapes.resize(1);
        node.node_param.in_shapes[0]    = param.in_shapes[0];
        node.node_param.out_shapes[0]   = param.out_shapes[0];
        int inHeight                    = param.in_shapes[0].Height;
        int inChannel                   = param.in_shapes[0].Channel;
        int inWidth                     = param.in_shapes[0].Width;
        int inBatch                     = param.in_shapes[0].Batch;

        //bottom_src memory
        node.node_param.bottom_shape    = {inBatch, inChannel, inHeight, inWidth};
        node.node_param.src_bottom_md   = memory::desc(node.node_param.bottom_shape, dnnDataType, data_order);

        print_dnnl_memory_shape(node.node_param.bottom_shape, "bottom_shape");
        print_dnnl_memory_shape(node.node_param.top_shape, "top_shape");

        //op
        node.node_param.activate_type   = get_op_mapped_activition_type(param.activate_type);
        node.node_param.alpha           = param.alpha;
        node.node_param.beta            = param.beta;
        dnnl::eltwise_forward::desc eltwise_desc = eltwise_forward::desc(prop_kind::forward_inference,
                                                                        node.node_param.activate_type, 
                                                                        node.node_param.src_bottom_md, 
                                                                        node.node_param.alpha,
                                                                        node.node_param.beta);
        node.node_param.eltwise_pdesc       = eltwise_forward::primitive_desc(eltwise_desc, BrixLab::graph_eng);
        // top &memory
        node.node_param.top_md        = node.node_param.eltwise_pdesc.dst_desc();
        node.node_param.top_memory    = memory(node.node_param.top_md, BrixLab::graph_eng);
        node.node_param.top_shape           = node.node_param.top_md.dims();
        node.node_param.inference_forward   = OP_activation_inference_forward;        
        LOG(DEBUG_INFO)<<"eltwise_setup done!\n"<<std::endl;
        return node;
    }
    INSTANCE_LAYEROP(activation);

    template<typename DType>
    void OP_binary_inference_forward(strNodeParam<DType> *node, graphSetLink<DType> &g_net){
        node->binary_memory[0]      = g_net[node->inputs[0]]->node_param.top_memory;
        print_dnnl_memory_shape(node->binary_memory[0].get_desc().dims(), "binary_0_shape");
        if(node->Custumter_){
            node->binary_memory[1]  = node->binary_custum;
        }else{
            node->binary_memory[1]  = g_net[node->inputs[1]]->node_param.top_memory;
        }
        
        node->op_args               = {{DNNL_ARG_SRC_0, node->binary_memory[0]},
                                        {DNNL_ARG_SRC_1, node->binary_memory[1]},
                                        {DNNL_ARG_DST, node->top_memory}};
        binary(node->binary_pdesc).execute(BrixLab::graph_stream, node->op_args);
        #ifdef USE_DUMP_FILE
        float *dst  = (float*)node->top_memory.get_data_handle();
        size_t size = node->top_memory.get_desc().get_size();
        dump_data_to_file(dst, size / 4, node->node_name.c_str(), node->top_memory.get_desc().dims());
        #endif
        LOG(DEBUG_INFO)<<"binary inference done!\n"<<std::endl;
    }
    template<typename DType>
    strLayerNode<DType> OP_binary_layer_setup(const strParam<DType> &param){
        strLayerNode<DType> node(strNodeParam<DType>(OP_type::BINARY_OP));
        LOG(DEBUG_INFO)<<get_quantized_type(param.quantized_type)<<",op: "<<get_mapped_op_string(param.op_type);
        node.node_param.node_name       = param.node_name;
        node.node_param.op_type         = param.op_type;
        QUANITIZED_TYPE quantized_type  = param.quantized_type;
        memory::data_type dnnDataType   = dt::f32;
        if(quantized_type == BrixLab::QUANITIZED_TYPE::UINT8_QUANTIZED){
            dnnDataType = dt::u8;
        }
        dnnl::memory::format_tag data_order     = tag::nchw;
        LOG_CHECK(param.in_shapes[0].format == param.out_shapes[0].format);
        LOG_CHECK(param.inIndexs.size()==2)<<"CHECK INPUTS SIZE ERROR";
        node.node_param.inputs.resize(param.inIndexs.size());
        node.node_param.inputs              = param.inIndexs;
        node.node_param.outputs.resize(param.outIndexs.size());
        node.node_param.outputs             = param.outIndexs;
        LOG_CHECK(param.in_shapes.size()==2)<<"CHECK_INPUTS";
        LOG_CHECK(param.out_shapes.size()==1)<<"CHECK_OUTPUTS";
        node.node_param.out_shapes.resize(1);
        node.node_param.out_shapes[0]   = param.out_shapes[0];
        node.node_param.in_shapes.resize(2);
        node.node_param.in_shapes[0]    = param.in_shapes[0];
        node.node_param.in_shapes[1]    = param.in_shapes[1];
        int inHeight                    = param.in_shapes[0].Height;
        int inChannel                   = param.in_shapes[0].Channel;
        int inWidth                     = param.in_shapes[0].Width;
        int inBatch                     = param.in_shapes[0].Batch;

        node.node_param.bottom_shape    = {inBatch, inChannel, inHeight, inWidth};
        node.node_param.src_bottom_md   = memory::desc(node.node_param.bottom_shape, dnnDataType, data_order);
        node.node_param.Custumter_      = param.custumter_data_;
        if(param.custumter_data_){
            node.node_param.binary_custum   = memory(node.node_param.src_bottom_md, BrixLab::graph_eng);
            DType* custum_data_             = (DType* )node.node_param.binary_custum.get_data_handle();
            dnnl::memory::dims custum_dims  = node.node_param.binary_custum.get_desc().dims();
            print_dnnl_memory_shape(custum_dims, "binary_custum");
            ReshapeExpandChannelMemory(param.binary_custum_data_, custum_data_, custum_dims);
        }

        LOG(DEBUG_INFO)<<"node.node_param.Custumter_: "<<node.node_param.Custumter_;
        
        for(unsigned int ii = 0; ii < param.inIndexs.size(); ii++){
            node.node_param.inputs.push_back(param.inIndexs[ii]);
            node.node_param.binary_md.push_back(node.node_param.src_bottom_md);
        }
        node.node_param.binary_memory.resize(2);
        // top &memory
        node.node_param.top_shape       = {inBatch, inChannel, inHeight, inWidth};
        node.node_param.top_md          = memory::desc(node.node_param.top_shape, dnnDataType, data_order);
        node.node_param.top_memory      = memory(node.node_param.top_md, BrixLab::graph_eng);

        print_dnnl_memory_shape(node.node_param.bottom_shape, "bottom_shape");
        print_dnnl_memory_shape(node.node_param.top_shape, "top_shape");
        //op
        node.node_param.binary_type         = get_op_mapped_binary_type(param.binary_type);
        dnnl::binary::desc binary_desc      = binary::desc(node.node_param.binary_type, 
                                                            node.node_param.binary_md[0], 
                                                            node.node_param.binary_md[1], 
                                                            node.node_param.top_md);
        
        if(param.fused_ops){
            node.node_param.post_param      = get_posts_opsMap(param.fused_act_type);
            node.node_param.p_ops.append_eltwise(node.node_param.post_param.scale, 
                                            node.node_param.post_param.posts_op, 
                                            node.node_param.post_param.alpha,
                                            node.node_param.post_param.beta);
            node.node_param.p_attr.set_post_ops(node.node_param.p_ops);
            node.node_param.binary_pdesc    = binary::primitive_desc(binary_desc, node.node_param.p_attr, BrixLab::graph_eng);
        }else{
            node.node_param.binary_pdesc    = binary::primitive_desc(binary_desc, BrixLab::graph_eng);
        }
        node.node_param.inference_forward   = OP_binary_inference_forward;
        LOG(DEBUG_INFO)<<"binary_op setup done!\n"<<std::endl;
        return node;
    }
    INSTANCE_LAYEROP(binary);

    template<typename DType>
    void OP_spaceTransposed_inference_forward(strNodeParam<DType> *node, graphSetLink<DType> &g_net){
        node->src_bottom_memory     = g_net[node->inputs[0]]->node_param.top_memory;
        DType*  src_data            = (DType*)node->src_bottom_memory.get_data_handle();
        DType*  dst_data            = (DType*)node->top_memory.get_data_handle();
        int block_size              = 1;
        for(unsigned int ii = 0; ii < node->block_shape.size(); ii++){
            block_size          *= node->block_shape[ii];
        }
        const int BSH           = node->block_shape[0];
        const int BSW           = node->block_shape[1];
        const int inB           = node->in_shapes[0].Batch;
        const int inC           = node->in_shapes[0].Channel;
        const int inH           = node->in_shapes[0].Height;
        const int inW           = node->in_shapes[0].Width;
        if(node->perm_type == DATA_PERMUTES_TYPE::Batch_To_SapceND){
            DType*  temp_data   = (DType*)xcalloc(product_dnnl_memory_shape(node->src_bottom_memory.get_desc().dims()), 
                                                                                sizeof(DType));
            // Reshape new Dims[b, c, h, w]->[bsh, bsw, b/ bs, c, h, w]
            memory::dims temp_d;
            temp_d.push_back(node->block_shape[0]);
            temp_d.push_back(node->block_shape[1]);
            temp_d.push_back(inB / block_size);
            temp_d.push_back(inC);
            temp_d.push_back(inH);
            temp_d.push_back(inW);
            // Permute Dims[bsh, bsw, b/ bs, c, h, w] ->[b/bs, c, h, bsh,w,bsw]
            const int num_axes_ = temp_d.size();
            std::vector<int> old_steps_(num_axes_, 1);
            std::vector<int> new_steps_(num_axes_, 1);
            std::vector<int> permute_order_  = {2, 3, 4, 0, 5, 1};
            memory::dims temp_p;
            for(unsigned int i = 0; i < permute_order_.size(); i++){
                temp_p.push_back(temp_d[permute_order_[i]]);
            }
            for (int i = 0; i < num_axes_; ++i) {
                if (i == num_axes_ - 1) {
                    new_steps_[i] = 1;
                    old_steps_[i] = 1;
                } else {
                    new_steps_[i] = dims_count(temp_p, i + 1, num_axes_ - 1);
                    old_steps_[i] = dims_count(temp_d, i + 1, num_axes_ - 1);
                }
            }
            const int* permute_order    = permute_order_.data();
            const int* old_steps        = old_steps_.data();
            const int* new_steps        = new_steps_.data();
            PermuteMemory(src_data, temp_data, temp_d, temp_p, num_axes_, permute_order,old_steps, new_steps);
            // Reshape out Dims[b/bs, c, h, bsh,w,bsw]->[b/bs, c, h * bsh, w * bsw]
            temp_d.clear();
            temp_d.push_back(inB / block_size);
            temp_d.push_back(inC);
            temp_d.push_back(inH * BSH);
            temp_d.push_back(inW * BSW);
            // Crop dst
            const int SH    = node->crop_size[0];
            const int EH    = temp_d[2] - node->crop_size[1];
            const int SW    = node->crop_size[2];
            const int EW    = temp_d[3] - node->crop_size[3];
            CropMemory(temp_data, dst_data, temp_d, node->top_shape, SH, EH, SW, EW);
            // End
            #ifdef USE_DUMP_FILE
            float *dst  = (float*)node->top_memory.get_data_handle();
            size_t size = node->top_memory.get_desc().get_size();
            dump_data_to_file(dst, size / 4, node->node_name.c_str(), node->top_memory.get_desc().dims());
            #endif
            free(temp_data);
        }else if(node->perm_type == DATA_PERMUTES_TYPE::Space_To_BatchND){
            const int height_pad    = node->crop_size[0] + inH + node->crop_size[1];
            const int width_pad     = node->crop_size[2] + inW + node->crop_size[3];
            const int mem_size      = product_dnnl_memory_shape({inB, inC, height_pad, width_pad});
            DType*  temp_data       = (DType*)xcalloc(mem_size, sizeof(DType));
            const int SH            = node->crop_size[0];
            const int EH            = height_pad - node->crop_size[1];
            const int SW            = node->crop_size[2];
            const int EW            = width_pad - node->crop_size[3];
            //Padding zero Dims[b, c, n_h, n_W]:temp_data
            fillMemory(src_data, temp_data, node->bottom_shape, {inB, inC, height_pad, width_pad}, SH, EH, SW, EW);
            // Reshape new Dims[b, c, n_h, n_W]->[b,c, n_h/bsh, bsh, n_w/bsw, bsw]:temp_data
            memory::dims temp_d;
            temp_d.push_back(inB);
            temp_d.push_back(inC);
            temp_d.push_back(height_pad / BSH);
            temp_d.push_back(BSH);
            temp_d.push_back(width_pad / BSW);
            temp_d.push_back(BSW);
            //Permute new Dims[b,c, n_h/bsh, bsh, n_w/bsw, bsw]->[bsh, bsw, b, c, n_h/bsh, n_w/bsw]
            const int num_axes_     = temp_d.size();
            std::vector<int> old_steps_(num_axes_, 1);
            std::vector<int> new_steps_(num_axes_, 1);
            std::vector<int> permute_order_ = {3, 5, 0, 1, 2, 4};
            memory::dims temp_p;
            for(unsigned int i = 0; i < permute_order_.size(); i++){
                temp_p.push_back(temp_d[permute_order_[i]]);
            }
            for (int i = 0; i < num_axes_; ++i) {
                if (i == num_axes_ - 1) {
                    new_steps_[i] = 1;
                    old_steps_[i] = 1;
                } else {
                    new_steps_[i] = dims_count(temp_p, i + 1, num_axes_ - 1);
                    old_steps_[i] = dims_count(temp_d, i + 1, num_axes_ - 1);
                }
            }
            const int* permute_order    = permute_order_.data();
            const int* old_steps        = old_steps_.data();
            const int* new_steps        = new_steps_.data();
            PermuteMemory(temp_data, dst_data, temp_d, temp_p, num_axes_, permute_order,old_steps, new_steps);
            // End
            #ifdef USE_DUMP_FILE
            float *dst  = (float*)node->top_memory.get_data_handle();
            size_t size = node->top_memory.get_desc().get_size();
            dump_data_to_file(dst, size / 4, node->node_name.c_str(), node->top_memory.get_desc().dims());
            #endif
            free(temp_data);
        }else{
            LOG(FATAL_ERROR)<<"No Support Data Permute Type Error: "<<node->perm_type;
        }
        LOG(DEBUG_INFO)<<"data space permute inference done!\n"<<std::endl;
    }
    template<typename DType>
    strLayerNode<DType> OP_spaceTransposed_layer_setup(const strParam<DType> &param){
        strLayerNode<DType> node(strNodeParam<DType>(OP_type::SPACE_PERMUTES));
        LOG(DEBUG_INFO)<<get_quantized_type(param.quantized_type)<<",op: "<<get_mapped_op_string(param.op_type);
        node.node_param.node_name       = param.node_name;
        node.node_param.op_type         = param.op_type;
        QUANITIZED_TYPE quantized_type  = param.quantized_type;
        memory::data_type dnnDataType   = dt::f32;
        if(quantized_type == BrixLab::QUANITIZED_TYPE::UINT8_QUANTIZED){
            dnnDataType = dt::u8;
        }
        dnnl::memory::format_tag data_order = tag::nchw;
        LOG_CHECK(param.in_shapes[0].format == param.out_shapes[0].format);
        LOG_CHECK(param.inIndexs.size()==3)<<"CHECK INPUTS SIZE ERROR";
        node.node_param.inputs.resize(param.inIndexs.size());
        node.node_param.inputs              = param.inIndexs;
        node.node_param.outputs.resize(param.outIndexs.size());
        node.node_param.outputs             = param.outIndexs;
        LOG_CHECK(param.in_shapes.size()==1)<<"CHECK_INPUTS ERROR";
        LOG_CHECK(param.out_shapes.size()==1)<<"CHECK_OUTPUTS ERROR";
        node.node_param.out_shapes.resize(1);
        node.node_param.out_shapes[0]   = param.out_shapes[0];
        node.node_param.in_shapes.resize(1);
        node.node_param.in_shapes[0]    = param.in_shapes[0];
        int inHeight                    = param.in_shapes[0].Height;
        int inChannel                   = param.in_shapes[0].Channel;
        int inWidth                     = param.in_shapes[0].Width;
        int inBatch                     = param.in_shapes[0].Batch;

        int outHeight                   = param.out_shapes[0].Height;
        int outChannel                  = param.out_shapes[0].Channel;
        int outWidth                    = param.out_shapes[0].Width;
        int outBatch                    = param.out_shapes[0].Batch;

        node.node_param.bottom_shape    = {inBatch, inChannel, inHeight, inWidth};
        node.node_param.src_bottom_md   = memory::desc(node.node_param.bottom_shape, dnnDataType, data_order);
        
        for(unsigned int ii = 0; ii < param.inIndexs.size(); ii++){
            node.node_param.inputs.push_back(param.inIndexs[ii]);
        }
        // top &memory
        node.node_param.top_shape       = {outBatch, outChannel, outHeight, outWidth};
        node.node_param.top_md          = memory::desc(node.node_param.top_shape, dnnDataType, data_order);
        node.node_param.top_memory      = memory(node.node_param.top_md, BrixLab::graph_eng);
        node.node_param.perm_type       = param.perm_type;

        print_dnnl_memory_shape(node.node_param.bottom_shape, "bottom_shape");
        print_dnnl_memory_shape(node.node_param.top_shape, "top_shape");
        // op
        node.node_param.block_shape.resize(param.block_shape.size());
        node.node_param.block_shape         = param.block_shape;
        node.node_param.crop_size.resize(param.crop_size.size());
        node.node_param.crop_size           = param.crop_size;
        node.node_param.inference_forward   = OP_spaceTransposed_inference_forward;
        LOG(DEBUG_INFO)<<"data space permute setup done!\n"<<std::endl;
        return node;
    }
    INSTANCE_LAYEROP(spaceTransposed);



    template<typename DType>
    NetGraph<DType>::NetGraph(const int &inH, const int &inW, const int &size, 
                const std::string &tflite_path):input_w(inW), input_h(inH), 
                graph_state(graphSetLink<DType>(0, 0)), graph_size(size),_tflite_model(nullptr), 
                tflite_file(tflite_path){
    }

    template<typename DType>
    int NetGraph<DType>::get_graph_size() const{
        return graph_size;
    }

    template<typename DType>
    int NetGraph<DType>::get_GraphinWidth() const{
        return input_w;
    }

    template<typename DType>
    int NetGraph<DType>::get_GraphinHeight() const{
        return input_h;
    }

    template<typename DType>
    void NetGraph<DType>::network_predict(){
        strLayerNode<DType> *layer_node     = graph_state.head;
        while(layer_node != nullptr){
            if(layer_node->node_param.op_type == OP_type::DATA_INPUTS){
                LOG(DEBUG_INFO)<<"DATA_INPUTS";
                #ifdef USE_DUMP_FILE
                strNodeParam<DType> *node = &(layer_node->node_param);
                float *dst  = (float*)node->top_memory.get_data_handle();
                size_t size = node->top_memory.get_desc().get_size();
                dump_data_to_file(dst, size / 4, "DATA_INPUTS", node->top_memory.get_desc().dims());
                #endif
                layer_node                  = layer_node->next;
                continue;
            }
            LOG(DEBUG_INFO)<<layer_node->node_param.node_name<<": start inference!";
            layer_node->node_param.inference_forward(&(layer_node->node_param), graph_state);
            layer_node                      = layer_node->next;
        }
        graph_stream.wait();
    }

    template<typename DType>
    strLayerNode<DType>* NetGraph<DType>::getGraphOutput(){
        return graph_state.tail;
    }

    template<typename DType>
    void NetGraph<DType>::make_graph(const NetT<DType>& g_net, DType *input, const dnnl::memory::format_tag &tag_type){
        for(unsigned int ii = 0; ii < g_net.layer_ops.size(); ii++){
            strParam<DType> param = g_net.layer_ops[ii];
            std::string OP_name = get_mapped_op_string(param.op_type) + "_layer_setup";
            strLayerNode<DType> node(param.op_type);
            if(param.op_type == OP_type::DATA_INPUTS){
                int batch   = param.in_shapes[0].Batch;
                int channel = param.in_shapes[0].Channel;
                int Height  = param.in_shapes[0].Height;
                int Width   = param.in_shapes[0].Width;
                node.node_param.top_memory  = dnnl::memory({{batch,channel,Height, Width}, dt::f32, tag_type}, BrixLab::graph_eng);
                node.node_param.op_type     = DATA_INPUTS;
                write_to_dnnl_memory(input, node.node_param.top_memory);
                graph_insert(graph_state, &node);
                continue;
            }
            if(param.quantized_type == BrixLab::QUANITIZED_TYPE::FLOAT32_REGULAR){
                node = getSetupFunc(OP_name)(param);
                graph_insert(graph_state, &node);
            }else{
                LOG(FATAL_ERROR)<<"no support quantized_type: "<<OP_name;
            }            
        }
    }

    template<typename DType>
    void NetGraph<DType>::make_netParamfromTflite(const std::string &tflite_file){

        std::ifstream inputFile(tflite_file, std::ios::binary);
        inputFile.seekg(0, std::ios::end);
        const auto size = inputFile.tellg();
        inputFile.seekg(0, std::ios::beg);

        char* buffer = new char[size];
        inputFile.read(buffer, size);
        inputFile.close();

        // verify model
        flatbuffers::Verifier verify((uint8_t*)buffer, size);
        if (!tflite::VerifyModelBuffer(verify)) {
            LOG(FATAL_ERROR) << "TFlite model version ERROR!";
        }

        _tflite_model = tflite::UnPackModel(buffer);
        delete[] buffer;
    }

    template<typename DType>
    void NetGraph<DType>::printf_netGraph(){
        LOG_CHECK(graph_state.graph_size>0)<<"graph_state should has layer_nodes";
        strLayerNode<DType> *layer_node    = graph_state.head;  
        while(layer_node != nullptr){
            OP_type type        = layer_node->node_param.op_type;
            std::string OP_name = get_mapped_op_string(type);
            LOG(DEBUG_INFO)<<"node op name: "<<OP_name<<", layer name: "<<layer_node->node_param.node_name.c_str();
            layer_node          = layer_node->next;
        }
    }

    template<typename DType>
    NetT<DType> NetGraph<DType>::tfliteConvertGraphList(){
        if(_tflite_model == nullptr){
            make_netParamfromTflite(tflite_file);
        }
        NetT<DType> g_net;
        const auto& tfliteOpSet = _tflite_model->operator_codes;
        const auto subGraphsSize      = _tflite_model->subgraphs.size();
        const auto& tfliteModelBuffer = _tflite_model->buffers;

        // check whether this tflite model is quantization model
        // use the weight's data type of Conv2D|DepthwiseConv2D to decide quantizedModel mode
        bool quantizedModel = true;
        for (unsigned int i = 0; i < subGraphsSize; ++i) {
            const auto& ops     = _tflite_model->subgraphs[i]->operators;
            const auto& tensors = _tflite_model->subgraphs[i]->tensors;
            const int opNums    = ops.size();
            for (int j = 0; j < opNums; ++j) {
                const int opcodeIndex = ops[j]->opcode_index;
                const auto opCode     = tfliteOpSet[opcodeIndex]->builtin_code;
                if (opCode == tflite::BuiltinOperator_CONV_2D || opCode == tflite::BuiltinOperator_DEPTHWISE_CONV_2D) {
                    const int weightIndex    = ops[j]->inputs[1];
                    const auto& weightTensor = tensors[weightIndex];
                    quantizedModel           = weightTensor->type == tflite::TensorType_UINT8;
                    if(weightTensor->type == tflite::TensorType_INT8){
                        LOG(FATAL_ERROR)<< "***DO NOT SUPPORT Tflite [INT8] quantized model now***";
                    }
                    if (!quantizedModel)
                        break;
                }
            }
        }
        //auto& buffers = _tflite_model->buffers;
        for (unsigned int i = 0; i < subGraphsSize; ++i) {
            const auto& ops     = _tflite_model->subgraphs[i]->operators;
            const auto& tensors = _tflite_model->subgraphs[i]->tensors;
            std::vector<std::string> tensor_names;
            tensor_names.resize(tensors.size());
            // set const
            std::vector<bool> extractedTensors(_tflite_model->subgraphs[i]->tensors.size(), false);
            // set input, maybe the inputs size should be 1 in one subgraphs.
            for (const auto index : _tflite_model->subgraphs[i]->inputs) {
                strParam<DType> input_OpT;
                const auto& inputTensor = tensors[index];
                input_OpT.in_shapes.resize(1);
                input_OpT.node_name     = inputTensor->name;
                input_OpT.op_type       = OP_type::DATA_INPUTS;
                input_OpT.in_shapes[0].format      = TENSOR_FORMATE::NHWC;
                input_OpT.in_shapes[0].Batch       = inputTensor->shape[0];
                input_OpT.in_shapes[0].Channel     = inputTensor->shape[3];
                input_OpT.in_shapes[0].Height      = inputTensor->shape[1];
                input_OpT.in_shapes[0].Width       = inputTensor->shape[2];
                g_net.layer_ops.push_back(input_OpT);
            }
            // set output names
            for (unsigned int k = 0; k < _tflite_model->subgraphs[i]->outputs.size(); ++k) {
                g_net.output_name.push_back(tensors[_tflite_model->subgraphs[i]->outputs[k]]->name);
            }
            // tensor names
            for (const auto& tensor : tensors) {
                g_net.tensorName.push_back(tensor->name);
            }
            const int opNums = ops.size();
            for (int j = 0; j < opNums; ++j) {
                const int opcodeIndex = ops[j]->opcode_index;
                const auto opCode     = tfliteOpSet[opcodeIndex]->builtin_code;
                strParam<DType> New_OP;
                auto creator = liteOpConvertMapKit<DType>::get()->search(opCode);
                LOG_CHECK(creator) << "NOT_SUPPORTED_OP: [ " << tflite::EnumNameBuiltinOperator(opCode) << " ]";

                // tflite op to onednn op
                New_OP.node_name    = tensors[ops[j]->outputs[0]]->name;
                New_OP.op_type      = creator->opType(quantizedModel);
                
                // set default input output index
                New_OP.inIndexs.resize(ops[j]->inputs.size());
                New_OP.outIndexs.resize(ops[j]->outputs.size());
                for (unsigned int i = 0; i < ops[j]->inputs.size(); i++) {
                    New_OP.inIndexs[i]  = ops[j]->inputs[i];
                }
                for (unsigned int i = 0; i < ops[j]->outputs.size(); i++) {
                    New_OP.outIndexs[i] = ops[j]->outputs[i];
                }
                // Run actual conversion
                creator->run(&New_OP, ops[j], tensors, tfliteModelBuffer, tfliteOpSet, quantizedModel);
                g_net.layer_ops.push_back(New_OP);
            }

            for(unsigned int i = 0; i < tensors.size(); i++){
                tensor_names[i] = tensors[i]->name;
            }

            for(unsigned int i = 0; i < g_net.layer_ops.size(); i++){
                std::vector<int>temp_index;
                temp_index.resize(g_net.layer_ops[i].inIndexs.size());
                temp_index      = g_net.layer_ops[i].inIndexs;
                for(unsigned int ii = 0; ii <temp_index.size();ii++){
                    g_net.layer_ops[i].inIndexs[ii]  = get_net_index_by_name(g_net.layer_ops, tensor_names[temp_index[ii]]);
                }
                temp_index.resize(g_net.layer_ops[i].outIndexs.size());
                temp_index      = g_net.layer_ops[i].outIndexs;
                for(unsigned int ii = 0; ii <temp_index.size();ii++){
                    g_net.layer_ops[i].outIndexs[ii]  = get_net_index_by_name(g_net.layer_ops, tensor_names[temp_index[ii]]);
                }
            }
        }
        
        return g_net;
    }

    template<typename DType>
    NetGraph<DType>::~NetGraph(){
        
    }

    INSTANEC_CLASSNET(NetGraph);

    LayerFloatSetup getSetupFunc(const std::string &func_name){
        if(func_name == "OP_convolution_layer_setup"){
            return OP_convolution_layer_setup;
        }else if(func_name == "OP_deconvolution_layer_setup"){
            return OP_deconvolution_layer_setup;
        }else if(func_name == "OP_activation_layer_setup"){
            return OP_activation_layer_setup;
        }else if(func_name == "OP_innerproduct_layer_setup"){
            return OP_innerproduct_layer_setup;
        }else if(func_name == "OP_resample_layer_setup"){
            return OP_resample_layer_setup;
        }else if(func_name == "OP_sum_layer_setup"){
            return OP_sum_layer_setup;
        }else if(func_name == "OP_concat_layer_setup"){
            return OP_concat_layer_setup;
        }else if(func_name == "OP_pooling_layer_setup"){
            return OP_pooling_layer_setup;
        }else if(func_name == "OP_batchnorm_layer_setup"){
            return OP_batchnorm_layer_setup;
        }else if(func_name == "OP_binary_layer_setup"){
            return OP_binary_layer_setup;
        }else if(func_name == "OP_spaceTransposed_layer_setup"){
            return OP_spaceTransposed_layer_setup;
        }else{
            LOG(FATAL_ERROR)<<"NO SUPPORT OPS";
            return nullptr;
        }
    }

    LayerUint8Setup getSetupUintFunc(const std::string &func_name){
        if(func_name == "OP_convolution_layer_setup"){
            return OP_convolution_layer_setup;
        }else if(func_name == "OP_deconvolution_layer_setup"){
            return OP_deconvolution_layer_setup;
        }else if(func_name == "OP_activation_layer_setup"){
            return OP_activation_layer_setup;
        }else if(func_name == "OP_innerproduct_layer_setup"){
            return OP_innerproduct_layer_setup;
        }else if(func_name == "OP_resample_layer_setup"){
            return OP_resample_layer_setup;
        }else if(func_name == "OP_sum_layer_setup"){
            return OP_sum_layer_setup;
        }else if(func_name == "OP_concat_layer_setup"){
            return OP_concat_layer_setup;
        }else if(func_name == "OP_pooling_layer_setup"){
            return OP_pooling_layer_setup;
        }else if(func_name == "OP_batchnorm_layer_setup"){
            return OP_batchnorm_layer_setup;
        }else if(func_name == "OP_binary_layer_setup"){
            return OP_binary_layer_setup;
        }else{
            LOG(FATAL_ERROR)<<"NO SUPPORT OPS";
            return nullptr;
        }
    }

} // namespace BrixLab

